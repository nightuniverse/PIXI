#!/usr/bin/env python3
"""
ê°œì„ ëœ ìŠ¤íƒ€íŠ¸ì—… ìƒíƒœê³„ í¬ë¡¤ëŸ¬
ë” ì•ˆì •ì ì´ê³  ì‹¤ì œë¡œ ì‘ë™í•˜ëŠ” í¬ë¡¤ë§ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import asyncio
import json
import logging
import time
import random
from datetime import datetime
from typing import Dict, List, Optional, Any
from urllib.parse import urljoin, urlparse, quote
import re

import aiohttp
import pandas as pd
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('improved_ecosystem_crawler.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ImprovedEcosystemCrawler:
    def __init__(self):
        self.session = None
        self.browser = None
        self.page = None
        self.ecosystem_data = {
            'startups': [],
            'investors': [],
            'accelerators': [],
            'coworking_spaces': [],
            'events': [],
            'crawled_at': datetime.now().isoformat()
        }
        
        # User-Agent ëª©ë¡
        self.user_agents = [
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]
        
        # í¬ë¡¤ë§ ì„¤ì •
        self.delay_range = (3, 6)
        self.max_retries = 3
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(
            headless=True,
            args=[
                '--no-sandbox', 
                '--disable-dev-shm-usage', 
                '--disable-blink-features=AutomationControlled',
                '--disable-web-security',
                '--disable-features=VizDisplayCompositor'
            ]
        )
        self.page = await self.browser.new_page()
        
        # User-Agent ì„¤ì •
        await self.page.set_extra_http_headers({
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()
    
    async def random_delay(self, min_delay=None, max_delay=None):
        """ëœë¤ ì§€ì—°ìœ¼ë¡œ ì°¨ë‹¨ ë°©ì§€"""
        if min_delay is None:
            min_delay = self.delay_range[0]
        if max_delay is None:
            max_delay = self.delay_range[1]
        
        delay = random.uniform(min_delay, max_delay)
        await asyncio.sleep(delay)
    
    async def crawl_github_startups(self, max_results: int = 50) -> List[Dict]:
        """GitHubì—ì„œ ìŠ¤íƒ€íŠ¸ì—… ê´€ë ¨ ì €ì¥ì†Œ í¬ë¡¤ë§"""
        logger.info("GitHub ìŠ¤íƒ€íŠ¸ì—… ì €ì¥ì†Œ í¬ë¡¤ë§ ì‹œì‘")
        startups = []
        
        # GitHub íŠ¸ë Œë”© ì €ì¥ì†Œ ê²€ìƒ‰
        search_queries = [
            "startup",
            "ai startup", 
            "fintech",
            "healthtech",
            "edtech"
        ]
        
        for query in search_queries:
            try:
                logger.info(f"GitHub '{query}' ê²€ìƒ‰ ì¤‘...")
                
                search_url = f"https://github.com/search?q={quote(query)}&type=repositories&s=stars&o=desc"
                await self.page.goto(search_url, wait_until='networkidle')
                await self.random_delay()
                
                # ì €ì¥ì†Œ ëª©ë¡ ëŒ€ê¸°
                try:
                    await self.page.wait_for_selector('.repo-list-item', timeout=15000)
                except:
                    logger.warning(f"'{query}' ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    continue
                
                repo_items = await self.page.query_selector_all('.repo-list-item')
                
                for i, item in enumerate(repo_items[:max_results//len(search_queries)]):
                    try:
                        # ì €ì¥ì†Œëª…
                        name_elem = await item.query_selector('a[data-hydro-click]')
                        name = await name_elem.text_content() if name_elem else "Unknown"
                        
                        # ì„¤ëª…
                        desc_elem = await item.query_selector('p')
                        description = await desc_elem.text_content() if desc_elem else ""
                        
                        # ì–¸ì–´
                        lang_elem = await item.query_selector('[itemprop="programmingLanguage"]')
                        language = await lang_elem.text_content() if lang_elem else ""
                        
                        # ìŠ¤íƒ€ ìˆ˜
                        stars_elem = await item.query_selector('a[href*="/stargazers"]')
                        stars = await stars_elem.text_content() if stars_elem else "0"
                        
                        # ë§í¬
                        link_elem = await item.query_selector('a[data-hydro-click]')
                        link = await link_elem.get_attribute('href') if link_elem else ""
                        
                        startup_data = {
                            'name': name.strip(),
                            'description': description.strip()[:500],
                            'language': language.strip(),
                            'stars': stars.strip(),
                            'website': f"https://github.com{link}" if link.startswith('/') else link,
                            'category': query,
                            'source': 'GitHub',
                            'type': 'startup',
                            'crawled_at': datetime.now().isoformat()
                        }
                        
                        startups.append(startup_data)
                        logger.info(f"GitHub í¬ë¡¤ë§: {name[:50]}... ({stars} stars)")
                        
                    except Exception as e:
                        logger.error(f"ì €ì¥ì†Œ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                        continue
                
                await self.random_delay(2, 4)
                
            except Exception as e:
                logger.error(f"GitHub '{query}' í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                continue
        
        logger.info(f"GitHub í¬ë¡¤ë§ ì™„ë£Œ: {len(startups)}ê°œ ì €ì¥ì†Œ")
        return startups
    
    async def crawl_product_hunt(self, max_results: int = 30) -> List[Dict]:
        """Product Huntì—ì„œ ìŠ¤íƒ€íŠ¸ì—… ì œí’ˆ ì •ë³´ í¬ë¡¤ë§"""
        logger.info("Product Hunt ìŠ¤íƒ€íŠ¸ì—… ì œí’ˆ í¬ë¡¤ë§ ì‹œì‘")
        startups = []
        
        try:
            # Product Hunt í™ˆí˜ì´ì§€
            url = "https://www.producthunt.com/"
            await self.page.goto(url, wait_until='networkidle')
            await self.random_delay()
            
            # ì œí’ˆ ì¹´ë“œ ëŒ€ê¸°
            try:
                await self.page.wait_for_selector('[data-test="post-item"]', timeout=15000)
            except:
                logger.warning("Product Hunt ì œí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return startups
            
            product_items = await self.page.query_selector_all('[data-test="post-item"]')
            
            for i, item in enumerate(product_items[:max_results]):
                try:
                    # ì œí’ˆëª…
                    name_elem = await item.query_selector('h3, [data-test="post-name"]')
                    name = await name_elem.text_content() if name_elem else "Unknown"
                    
                    # ì„¤ëª…
                    desc_elem = await item.query_selector('p, [data-test="post-tagline"]')
                    description = await desc_elem.text_content() if desc_elem else ""
                    
                    # ì¹´í…Œê³ ë¦¬
                    category_elem = await item.query_selector('[data-test="post-topic"]')
                    category = await category_elem.text_content() if category_elem else ""
                    
                    # ë§í¬
                    link_elem = await item.query_selector('a')
                    link = await link_elem.get_attribute('href') if link_elem else ""
                    
                    startup_data = {
                        'name': name.strip(),
                        'description': description.strip()[:500],
                        'category': category.strip(),
                        'website': link if link.startswith('http') else f"https://www.producthunt.com{link}",
                        'source': 'Product Hunt',
                        'type': 'startup',
                        'crawled_at': datetime.now().isoformat()
                    }
                    
                    startups.append(startup_data)
                    logger.info(f"Product Hunt í¬ë¡¤ë§: {name[:50]}...")
                    
                except Exception as e:
                    logger.error(f"ì œí’ˆ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                    continue
                
        except Exception as e:
            logger.error(f"Product Hunt í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        
        logger.info(f"Product Hunt í¬ë¡¤ë§ ì™„ë£Œ: {len(startups)}ê°œ ì œí’ˆ")
        return startups
    
    async def crawl_medium_startups(self, max_pages: int = 3) -> List[Dict]:
        """Mediumì—ì„œ ìŠ¤íƒ€íŠ¸ì—… ê´€ë ¨ ê¸€ í¬ë¡¤ë§"""
        logger.info("Medium ìŠ¤íƒ€íŠ¸ì—… ê¸€ í¬ë¡¤ë§ ì‹œì‘")
        startups = []
        
        # Medium ìŠ¤íƒ€íŠ¸ì—… íƒœê·¸ ê²€ìƒ‰
        search_queries = [
            "startup",
            "entrepreneurship",
            "venture-capital",
            "tech-startup"
        ]
        
        for query in search_queries:
            try:
                logger.info(f"Medium '{query}' ê²€ìƒ‰ ì¤‘...")
                
                search_url = f"https://medium.com/tag/{query}"
                await self.page.goto(search_url, wait_until='networkidle')
                await self.random_delay()
                
                # ê¸€ ëª©ë¡ ëŒ€ê¸°
                try:
                    await self.page.wait_for_selector('article', timeout=15000)
                except:
                    logger.warning(f"'{query}' ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    continue
                
                articles = await self.page.query_selector_all('article')
                
                for i, article in enumerate(articles[:max_results//len(search_queries)]):
                    try:
                        # ì œëª©
                        title_elem = await article.query_selector('h2, h3')
                        title = await title_elem.text_content() if title_elem else "No Title"
                        
                        # ìš”ì•½
                        excerpt_elem = await article.query_selector('p')
                        excerpt = await excerpt_elem.text_content() if excerpt_elem else ""
                        
                        # ì‘ì„±ì
                        author_elem = await article.query_selector('[data-testid="authorName"]')
                        author = await author_elem.text_content() if author_elem else ""
                        
                        # ë§í¬
                        link_elem = await article.query_selector('a')
                        link = await link_elem.get_attribute('href') if link_elem else ""
                        
                        startup_data = {
                            'name': title.strip()[:100],
                            'description': excerpt.strip()[:500],
                            'author': author.strip(),
                            'website': link if link.startswith('http') else f"https://medium.com{link}",
                            'category': query,
                            'source': 'Medium',
                            'type': 'startup',
                            'crawled_at': datetime.now().isoformat()
                        }
                        
                        startups.append(startup_data)
                        logger.info(f"Medium í¬ë¡¤ë§: {title[:50]}...")
                        
                    except Exception as e:
                        logger.error(f"ê¸€ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                        continue
                
                await self.random_delay(2, 4)
                
            except Exception as e:
                logger.error(f"Medium '{query}' í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                continue
        
        logger.info(f"Medium í¬ë¡¤ë§ ì™„ë£Œ: {len(startups)}ê°œ ê¸€")
        return startups
    
    async def crawl_manual_startup_data(self) -> List[Dict]:
        """ìˆ˜ë™ìœ¼ë¡œ ìˆ˜ì§‘í•œ ì£¼ìš” ìŠ¤íƒ€íŠ¸ì—… ë°ì´í„°"""
        logger.info("ìˆ˜ë™ ìŠ¤íƒ€íŠ¸ì—… ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        
        manual_startups = [
            {
                "name": "OpenAI",
                "description": "AI ì—°êµ¬ ë° ê°œë°œ ê¸°ì—…, ChatGPT ê°œë°œì‚¬",
                "location": "San Francisco, CA",
                "industry": "Artificial Intelligence",
                "founded": "2015",
                "funding": "Series Unknown",
                "website": "https://openai.com",
                "source": "Manual Data"
            },
            {
                "name": "Stripe",
                "description": "ì˜¨ë¼ì¸ ê²°ì œ ì²˜ë¦¬ í”Œë«í¼",
                "location": "San Francisco, CA",
                "industry": "Fintech",
                "founded": "2010",
                "funding": "Series Unknown",
                "website": "https://stripe.com",
                "source": "Manual Data"
            },
            {
                "name": "Notion",
                "description": "í˜‘ì—… ë° ìƒì‚°ì„± ë„êµ¬",
                "location": "San Francisco, CA",
                "industry": "Productivity",
                "founded": "2013",
                "funding": "Series Unknown",
                "website": "https://notion.so",
                "source": "Manual Data"
            },
            {
                "name": "Figma",
                "description": "í˜‘ì—… ë””ìì¸ ë„êµ¬",
                "location": "San Francisco, CA",
                "industry": "Design",
                "founded": "2012",
                "funding": "Series Unknown",
                "website": "https://figma.com",
                "source": "Manual Data"
            },
            {
                "name": "Canva",
                "description": "ì˜¨ë¼ì¸ ë””ìì¸ í”Œë«í¼",
                "location": "Sydney, Australia",
                "industry": "Design",
                "founded": "2012",
                "funding": "Series Unknown",
                "website": "https://canva.com",
                "source": "Manual Data"
            }
        ]
        
        for startup in manual_startups:
            startup['type'] = 'startup'
            startup['crawled_at'] = datetime.now().isoformat()
        
        logger.info(f"ìˆ˜ë™ ìŠ¤íƒ€íŠ¸ì—… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(manual_startups)}ê°œ")
        return manual_startups
    
    async def crawl_manual_investor_data(self) -> List[Dict]:
        """ìˆ˜ë™ìœ¼ë¡œ ìˆ˜ì§‘í•œ ì£¼ìš” íˆ¬ìì ë°ì´í„°"""
        logger.info("ìˆ˜ë™ íˆ¬ìì ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        
        manual_investors = [
            {
                "name": "Sequoia Capital",
                "description": "ì‹¤ë¦¬ì½˜ë°¸ë¦¬ì˜ ëŒ€í‘œì ì¸ ë²¤ì²˜ìºí”¼íƒˆ",
                "location": "Menlo Park, CA",
                "focus": "Technology, Healthcare, Consumer",
                "website": "https://www.sequoiacap.com",
                "source": "Manual Data"
            },
            {
                "name": "Andreessen Horowitz",
                "description": "ê¸°ìˆ  ì¤‘ì‹¬ì˜ ë²¤ì²˜ìºí”¼íƒˆ",
                "location": "Menlo Park, CA",
                "focus": "Software, Internet, Mobile",
                "website": "https://a16z.com",
                "source": "Manual Data"
            },
            {
                "name": "Y Combinator",
                "description": "ìŠ¤íƒ€íŠ¸ì—… ì•¡ì…€ëŸ¬ë ˆì´í„°",
                "location": "Mountain View, CA",
                "focus": "Early-stage startups",
                "website": "https://www.ycombinator.com",
                "source": "Manual Data"
            }
        ]
        
        for investor in manual_investors:
            investor['type'] = 'investor'
            investor['crawled_at'] = datetime.now().isoformat()
        
        logger.info(f"ìˆ˜ë™ íˆ¬ìì ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(manual_investors)}ê°œ")
        return manual_investors
    
    async def crawl_all_improved_sources(self) -> Dict[str, Any]:
        """ëª¨ë“  ê°œì„ ëœ ì†ŒìŠ¤ì—ì„œ ë°ì´í„° í¬ë¡¤ë§"""
        logger.info("ê°œì„ ëœ ìƒíƒœê³„ ë°ì´í„° í¬ë¡¤ë§ ì‹œì‘")
        
        # 1. GitHub ìŠ¤íƒ€íŠ¸ì—… ì €ì¥ì†Œ
        github_startups = await self.crawl_github_startups()
        self.ecosystem_data['startups'].extend(github_startups)
        
        # 2. Product Hunt ì œí’ˆ
        product_hunt_startups = await self.crawl_product_hunt()
        self.ecosystem_data['startups'].extend(product_hunt_startups)
        
        # 3. Medium ìŠ¤íƒ€íŠ¸ì—… ê¸€
        medium_startups = await self.crawl_medium_startups()
        self.ecosystem_data['startups'].extend(medium_startups)
        
        # 4. ìˆ˜ë™ ìŠ¤íƒ€íŠ¸ì—… ë°ì´í„°
        manual_startups = await self.crawl_manual_startup_data()
        self.ecosystem_data['startups'].extend(manual_startups)
        
        # 5. ìˆ˜ë™ íˆ¬ìì ë°ì´í„°
        manual_investors = await self.crawl_manual_investor_data()
        self.ecosystem_data['investors'].extend(manual_investors)
        
        # ì¤‘ë³µ ì œê±°
        self._remove_duplicates()
        
        # í†µê³„ ì •ë³´ ì¶”ê°€
        self._add_statistics()
        
        logger.info("ê°œì„ ëœ ìƒíƒœê³„ ë°ì´í„° í¬ë¡¤ë§ ì™„ë£Œ")
        return self.ecosystem_data
    
    def _remove_duplicates(self):
        """ì¤‘ë³µ ë°ì´í„° ì œê±°"""
        logger.info("ì¤‘ë³µ ë°ì´í„° ì œê±° ì¤‘...")
        
        # íšŒì‚¬ëª… ê¸°ë°˜ ì¤‘ë³µ ì œê±°
        seen_names = set()
        unique_startups = []
        
        for startup in self.ecosystem_data['startups']:
            name_normalized = startup['name'].lower().strip()
            if name_normalized not in seen_names:
                seen_names.add(name_normalized)
                unique_startups.append(startup)
        
        self.ecosystem_data['startups'] = unique_startups
        
        logger.info(f"ì¤‘ë³µ ì œê±° í›„ ìŠ¤íƒ€íŠ¸ì—… ìˆ˜: {len(unique_startups)}")
    
    def _add_statistics(self):
        """í†µê³„ ì •ë³´ ì¶”ê°€"""
        stats = {
            'total_startups': len(self.ecosystem_data['startups']),
            'total_investors': len(self.ecosystem_data['investors']),
            'total_accelerators': len(self.ecosystem_data['accelerators']),
            'total_coworking_spaces': len(self.ecosystem_data['coworking_spaces']),
            'total_events': len(self.ecosystem_data['events']),
            'total_entities': sum([
                len(self.ecosystem_data['startups']),
                len(self.ecosystem_data['investors']),
                len(self.ecosystem_data['accelerators']),
                len(self.ecosystem_data['coworking_spaces']),
                len(self.ecosystem_data['events'])
            ])
        }
        
        self.ecosystem_data['statistics'] = stats
        logger.info(f"í†µê³„ ì •ë³´ ì¶”ê°€ ì™„ë£Œ: ì´ {stats['total_entities']}ê°œ ì—”í‹°í‹°")
    
    def save_data(self, filename: str = None):
        """ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"improved_ecosystem_data_{timestamp}.json"
        
        filepath = f"data/{filename}"
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(self.ecosystem_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            return None

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("ğŸš€ ê°œì„ ëœ ìŠ¤íƒ€íŠ¸ì—… ìƒíƒœê³„ íƒìƒ‰ í¬ë¡¤ëŸ¬ ì‹œì‘")
    
    async with ImprovedEcosystemCrawler() as crawler:
        try:
            # ì „ì²´ ê°œì„ ëœ ì†ŒìŠ¤ì—ì„œ ë°ì´í„° í¬ë¡¤ë§
            ecosystem_data = await crawler.crawl_all_improved_sources()
            
            # ë°ì´í„° ì €ì¥
            filename = crawler.save_data()
            
            if filename:
                logger.info(f"âœ… ê°œì„ ëœ í¬ë¡¤ë§ ì™„ë£Œ! ë°ì´í„° ì €ì¥ë¨: {filename}")
                
                # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
                stats = ecosystem_data.get('statistics', {})
                logger.info(f"ğŸ“Š ê°œì„ ëœ í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½:")
                logger.info(f"   - ìŠ¤íƒ€íŠ¸ì—…: {stats.get('total_startups', 0)}ê°œ")
                logger.info(f"   - íˆ¬ìì: {stats.get('total_investors', 0)}ê°œ")
                logger.info(f"   - ì•¡ì…€ëŸ¬ë ˆì´í„°: {stats.get('total_accelerators', 0)}ê°œ")
                logger.info(f"   - ì½”ì›Œí‚¹ ìŠ¤í˜ì´ìŠ¤: {stats.get('total_coworking_spaces', 0)}ê°œ")
                logger.info(f"   - ì´ë²¤íŠ¸: {stats.get('total_events', 0)}ê°œ")
                logger.info(f"   - ì´ ì—”í‹°í‹°: {stats.get('total_entities', 0)}ê°œ")
            else:
                logger.error("âŒ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨")
                
        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    asyncio.run(main())
