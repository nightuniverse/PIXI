#!/usr/bin/env python3
"""
ìŠ¤íƒ€íŠ¸ì—… ìƒíƒœê³„ ì¢…í•© íƒìƒ‰ í¬ë¡¤ëŸ¬
ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ìŠ¤íƒ€íŠ¸ì—…, íˆ¬ìì, ì•¡ì…€ëŸ¬ë ˆì´í„°, ì½”ì›Œí‚¹ ìŠ¤í˜ì´ìŠ¤ ë“±ì˜ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

import asyncio
import json
import logging
import time
import random
from datetime import datetime
from typing import Dict, List, Optional, Any
from urllib.parse import urljoin, urlparse, quote
import re

import aiohttp
import pandas as pd
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ecosystem_crawler.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class EcosystemCrawler:
    def __init__(self):
        self.session = None
        self.browser = None
        self.page = None
        self.ecosystem_data = {
            'startups': [],
            'investors': [],
            'accelerators': [],
            'coworking_spaces': [],
            'events': [],
            'crawled_at': datetime.now().isoformat()
        }
        
        # User-Agent ëª©ë¡
        self.user_agents = [
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]
        
        # í¬ë¡¤ë§ ì„¤ì •
        self.delay_range = (2, 5)  # ìš”ì²­ ê°„ ì§€ì—° ì‹œê°„ (ì´ˆ)
        self.max_retries = 3
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(
            headless=True,
            args=['--no-sandbox', '--disable-dev-shm-usage', '--disable-blink-features=AutomationControlled']
        )
        self.page = await self.browser.new_page()
        
        # User-Agent ì„¤ì •
        await self.page.set_extra_http_headers({
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()
    
    async def random_delay(self, min_delay=None, max_delay=None):
        """ëœë¤ ì§€ì—°ìœ¼ë¡œ ì°¨ë‹¨ ë°©ì§€"""
        if min_delay is None:
            min_delay = self.delay_range[0]
        if max_delay is None:
            max_delay = self.delay_range[1]
        
        delay = random.uniform(min_delay, max_delay)
        await asyncio.sleep(delay)
    
    async def crawl_techcrunch_startups(self, max_pages: int = 5) -> List[Dict]:
        """TechCrunchì—ì„œ ìŠ¤íƒ€íŠ¸ì—… ë‰´ìŠ¤ ë° ì •ë³´ í¬ë¡¤ë§"""
        logger.info("TechCrunch ìŠ¤íƒ€íŠ¸ì—… ì •ë³´ í¬ë¡¤ë§ ì‹œì‘")
        startups = []
        
        try:
            for page in range(1, max_pages + 1):
                url = f"https://techcrunch.com/category/startups/page/{page}/"
                logger.info(f"í˜ì´ì§€ {page} í¬ë¡¤ë§: {url}")
                
                await self.page.goto(url, wait_until='networkidle')
                await self.random_delay()
                
                # ê¸°ì‚¬ ëª©ë¡ ëŒ€ê¸°
                try:
                    await self.page.wait_for_selector('article', timeout=10000)
                except:
                    logger.warning(f"í˜ì´ì§€ {page}ì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    break
                
                articles = await self.page.query_selector_all('article')
                
                for article in articles:
                    try:
                        # ì œëª©
                        title_elem = await article.query_selector('h2, h3')
                        title = await title_elem.text_content() if title_elem else "No Title"
                        
                        # ë§í¬
                        link_elem = await article.query_selector('a')
                        link = await link_elem.get_attribute('href') if link_elem else ""
                        
                        # ìš”ì•½
                        excerpt_elem = await article.query_selector('p, div[class*="excerpt"]')
                        excerpt = await excerpt_elem.text_content() if excerpt_elem else ""
                        
                        # ì¹´í…Œê³ ë¦¬/íƒœê·¸
                        category_elem = await article.query_selector('[class*="category"], [class*="tag"]')
                        category = await category_elem.text_content() if category_elem else ""
                        
                        startup_data = {
                            'name': title.strip()[:100],
                            'description': excerpt.strip()[:500],
                            'website': link if link.startswith('http') else f"https://techcrunch.com{link}",
                            'category': category.strip(),
                            'source': 'TechCrunch',
                            'type': 'startup',
                            'crawled_at': datetime.now().isoformat()
                        }
                        
                        startups.append(startup_data)
                        logger.info(f"TechCrunch í¬ë¡¤ë§: {title[:50]}...")
                        
                    except Exception as e:
                        logger.error(f"ê¸°ì‚¬ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                        continue
                
                await self.random_delay(3, 6)  # í˜ì´ì§€ ê°„ ë” ê¸´ ì§€ì—°
                
        except Exception as e:
            logger.error(f"TechCrunch í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        
        logger.info(f"TechCrunch í¬ë¡¤ë§ ì™„ë£Œ: {len(startups)}ê°œ ìŠ¤íƒ€íŠ¸ì—…")
        return startups
    
    async def crawl_angel_list(self, search_queries: List[str] = None, max_results: int = 20) -> List[Dict]:
        """AngelListì—ì„œ ìŠ¤íƒ€íŠ¸ì—… ì •ë³´ í¬ë¡¤ë§"""
        if search_queries is None:
            search_queries = ["AI", "Fintech", "HealthTech", "EdTech", "SaaS"]
        
        logger.info("AngelList ìŠ¤íƒ€íŠ¸ì—… ì •ë³´ í¬ë¡¤ë§ ì‹œì‘")
        startups = []
        
        for query in search_queries:
            try:
                logger.info(f"AngelList '{query}' ê²€ìƒ‰ ì¤‘...")
                
                # AngelList ê²€ìƒ‰ URL
                search_url = f"https://angel.co/companies?keywords={quote(query)}"
                await self.page.goto(search_url, wait_until='networkidle')
                await self.random_delay()
                
                # ê²€ìƒ‰ ê²°ê³¼ ëŒ€ê¸°
                try:
                    await self.page.wait_for_selector('[data-testid="company-card"], .company-card', timeout=15000)
                except:
                    logger.warning(f"'{query}' ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    continue
                
                company_cards = await self.page.query_selector_all('[data-testid="company-card"], .company-card')
                
                for i, card in enumerate(company_cards[:max_results//len(search_queries)]):
                    try:
                        # íšŒì‚¬ëª…
                        name_elem = await card.query_selector('h3, .company-name')
                        name = await name_elem.text_content() if name_elem else "Unknown"
                        
                        # ì„¤ëª…
                        desc_elem = await card.query_selector('p, .description')
                        description = await desc_elem.text_content() if desc_elem else ""
                        
                        # ìœ„ì¹˜
                        location_elem = await card.query_selector('[class*="location"], .location')
                        location = await location_elem.text_content() if location_elem else ""
                        
                        # í€ë”© ì •ë³´
                        funding_elem = await card.query_selector('[class*="funding"], .funding')
                        funding = await funding_elem.text_content() if funding_elem else ""
                        
                        startup_data = {
                            'name': name.strip(),
                            'description': description.strip()[:500],
                            'location': location.strip(),
                            'funding': funding.strip(),
                            'category': query,
                            'source': 'AngelList',
                            'type': 'startup',
                            'crawled_at': datetime.now().isoformat()
                        }
                        
                        startups.append(startup_data)
                        logger.info(f"AngelList í¬ë¡¤ë§: {name[:50]}...")
                        
                    except Exception as e:
                        logger.error(f"íšŒì‚¬ ì¹´ë“œ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                        continue
                
                await self.random_delay(2, 4)
                
            except Exception as e:
                logger.error(f"AngelList '{query}' í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                continue
        
        logger.info(f"AngelList í¬ë¡¤ë§ ì™„ë£Œ: {len(startups)}ê°œ ìŠ¤íƒ€íŠ¸ì—…")
        return startups
    
    async def crawl_crunchbase_ecosystem(self, search_queries: List[str] = None, max_results: int = 30) -> List[Dict]:
        """Crunchbaseì—ì„œ ìƒíƒœê³„ ì •ë³´ í¬ë¡¤ë§"""
        if search_queries is None:
            search_queries = ["AI startups", "Fintech companies", "HealthTech", "EdTech", "SaaS"]
        
        logger.info("Crunchbase ìƒíƒœê³„ ì •ë³´ í¬ë¡¤ë§ ì‹œì‘")
        all_data = []
        
        for query in search_queries:
            try:
                logger.info(f"Crunchbase '{query}' ê²€ìƒ‰ ì¤‘...")
                
                search_url = f"https://www.crunchbase.com/search/organizations?query={quote(query)}"
                await self.page.goto(search_url, wait_until='networkidle')
                await self.random_delay()
                
                # ê²€ìƒ‰ ê²°ê³¼ ëŒ€ê¸°
                try:
                    await self.page.wait_for_selector('.result-card, .search-result', timeout=15000)
                except:
                    logger.warning(f"'{query}' ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    continue
                
                results = await self.page.query_selector_all('.result-card, .search-result')
                
                for i, result in enumerate(results[:max_results//len(search_queries)]):
                    try:
                        # íšŒì‚¬ëª…
                        name_elem = await result.query_selector('.result-card__title, .search-result__title')
                        name = await name_elem.text_content() if name_elem else "Unknown"
                        
                        # ì„¤ëª…
                        desc_elem = await result.query_selector('.result-card__description, .search-result__description')
                        description = await desc_elem.text_content() if desc_elem else ""
                        
                        # ìœ„ì¹˜
                        location_elem = await result.query_selector('.result-card__location, .search-result__location')
                        location = await location_elem.text_content() if location_elem else ""
                        
                        # í€ë”© ì •ë³´
                        funding_elem = await result.query_selector('.result-card__funding, .search-result__funding')
                        funding = await funding_elem.text_content() if funding_elem else ""
                        
                        # íšŒì‚¬ íƒ€ì… íŒë³„
                        company_type = self._determine_company_type(name, description, query)
                        
                        company_data = {
                            'name': name.strip(),
                            'description': description.strip()[:500],
                            'location': location.strip(),
                            'funding': funding.strip(),
                            'category': query,
                            'source': 'Crunchbase',
                            'type': company_type,
                            'crawled_at': datetime.now().isoformat()
                        }
                        
                        all_data.append(company_data)
                        logger.info(f"Crunchbase í¬ë¡¤ë§: {name[:50]}... ({company_type})")
                        
                    except Exception as e:
                        logger.error(f"ê²€ìƒ‰ ê²°ê³¼ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                        continue
                
                await self.random_delay(2, 4)
                
            except Exception as e:
                logger.error(f"Crunchbase '{query}' í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                continue
        
        logger.info(f"Crunchbase í¬ë¡¤ë§ ì™„ë£Œ: {len(all_data)}ê°œ í•­ëª©")
        return all_data
    
    def _determine_company_type(self, name: str, description: str, category: str) -> str:
        """íšŒì‚¬ íƒ€ì…ì„ íŒë³„í•˜ëŠ” í•¨ìˆ˜"""
        name_lower = name.lower()
        desc_lower = description.lower()
        category_lower = category.lower()
        
        # íˆ¬ìì ê´€ë ¨ í‚¤ì›Œë“œ
        investor_keywords = ['venture', 'capital', 'vc', 'investment', 'fund', 'angel', 'investor']
        if any(keyword in name_lower or keyword in desc_lower for keyword in investor_keywords):
            return 'investor'
        
        # ì•¡ì…€ëŸ¬ë ˆì´í„° ê´€ë ¨ í‚¤ì›Œë“œ
        accelerator_keywords = ['accelerator', 'incubator', 'studio', 'lab']
        if any(keyword in name_lower or keyword in desc_lower for keyword in accelerator_keywords):
            return 'accelerator'
        
        # ì½”ì›Œí‚¹ ìŠ¤í˜ì´ìŠ¤ ê´€ë ¨ í‚¤ì›Œë“œ
        coworking_keywords = ['coworking', 'space', 'hub', 'center', 'office']
        if any(keyword in name_lower or keyword in desc_lower for keyword in coworking_keywords):
            return 'coworking_space'
        
        # ê¸°ë³¸ì ìœ¼ë¡œëŠ” ìŠ¤íƒ€íŠ¸ì—…ìœ¼ë¡œ ë¶„ë¥˜
        return 'startup'
    
    async def crawl_startup_events(self, max_pages: int = 3) -> List[Dict]:
        """ìŠ¤íƒ€íŠ¸ì—… ì´ë²¤íŠ¸ ì •ë³´ í¬ë¡¤ë§"""
        logger.info("ìŠ¤íƒ€íŠ¸ì—… ì´ë²¤íŠ¸ ì •ë³´ í¬ë¡¤ë§ ì‹œì‘")
        events = []
        
        # Eventbrite ìŠ¤íƒ€íŠ¸ì—… ì´ë²¤íŠ¸ ê²€ìƒ‰
        try:
            search_url = "https://www.eventbrite.com/d/search/?q=startup"
            await self.page.goto(search_url, wait_until='networkidle')
            await self.random_delay()
            
            # ì´ë²¤íŠ¸ ì¹´ë“œ ëŒ€ê¸°
            try:
                await self.page.wait_for_selector('[data-testid="event-card"], .eds-event-card', timeout=15000)
            except:
                logger.warning("Eventbrite ì´ë²¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return events
            
            event_cards = await self.page.query_selector_all('[data-testid="event-card"], .eds-event-card')
            
            for i, card in enumerate(event_cards[:20]):
                try:
                    # ì´ë²¤íŠ¸ëª…
                    title_elem = await card.query_selector('h3, .eds-event-card__title')
                    title = await title_elem.text_content() if title_elem else "Unknown Event"
                    
                    # ë‚ ì§œ
                    date_elem = await card.query_selector('[class*="date"], .eds-event-card__date')
                    date = await date_elem.text_content() if date_elem else ""
                    
                    # ìœ„ì¹˜
                    location_elem = await card.query_selector('[class*="location"], .eds-event-card__location')
                    location = await location_elem.text_content() if location_elem else ""
                    
                    # ë§í¬
                    link_elem = await card.query_selector('a')
                    link = await link_elem.get_attribute('href') if link_elem else ""
                    
                    event_data = {
                        'name': title.strip(),
                        'date': date.strip(),
                        'location': location.strip(),
                        'website': f"https://www.eventbrite.com{link}" if link.startswith('/') else link,
                        'source': 'Eventbrite',
                        'type': 'event',
                        'crawled_at': datetime.now().isoformat()
                    }
                    
                    events.append(event_data)
                    logger.info(f"ì´ë²¤íŠ¸ í¬ë¡¤ë§: {title[:50]}...")
                    
                except Exception as e:
                    logger.error(f"ì´ë²¤íŠ¸ ì¹´ë“œ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                    continue
                
        except Exception as e:
            logger.error(f"Eventbrite í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        
        logger.info(f"ì´ë²¤íŠ¸ í¬ë¡¤ë§ ì™„ë£Œ: {len(events)}ê°œ ì´ë²¤íŠ¸")
        return events
    
    async def crawl_all_sources(self) -> Dict[str, Any]:
        """ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ë°ì´í„° í¬ë¡¤ë§"""
        logger.info("ì „ì²´ ìƒíƒœê³„ ë°ì´í„° í¬ë¡¤ë§ ì‹œì‘")
        
        # 1. TechCrunch ìŠ¤íƒ€íŠ¸ì—… ì •ë³´
        techcrunch_startups = await self.crawl_techcrunch_startups()
        self.ecosystem_data['startups'].extend(techcrunch_startups)
        
        # 2. AngelList ìŠ¤íƒ€íŠ¸ì—… ì •ë³´
        angel_list_startups = await self.crawl_angel_list()
        self.ecosystem_data['startups'].extend(angel_list_startups)
        
        # 3. Crunchbase ìƒíƒœê³„ ì •ë³´
        crunchbase_data = await self.crawl_crunchbase_ecosystem()
        
        # ë°ì´í„° íƒ€ì…ë³„ë¡œ ë¶„ë¥˜
        for item in crunchbase_data:
            if item['type'] == 'startup':
                self.ecosystem_data['startups'].append(item)
            elif item['type'] == 'investor':
                self.ecosystem_data['investors'].append(item)
            elif item['type'] == 'accelerator':
                self.ecosystem_data['accelerators'].append(item)
            elif item['type'] == 'coworking_space':
                self.ecosystem_data['coworking_spaces'].append(item)
        
        # 4. ìŠ¤íƒ€íŠ¸ì—… ì´ë²¤íŠ¸ ì •ë³´
        events = await self.crawl_startup_events()
        self.ecosystem_data['events'].extend(events)
        
        # ì¤‘ë³µ ì œê±°
        self._remove_duplicates()
        
        # í†µê³„ ì •ë³´ ì¶”ê°€
        self._add_statistics()
        
        logger.info("ì „ì²´ ìƒíƒœê³„ ë°ì´í„° í¬ë¡¤ë§ ì™„ë£Œ")
        return self.ecosystem_data
    
    def _remove_duplicates(self):
        """ì¤‘ë³µ ë°ì´í„° ì œê±°"""
        logger.info("ì¤‘ë³µ ë°ì´í„° ì œê±° ì¤‘...")
        
        # íšŒì‚¬ëª… ê¸°ë°˜ ì¤‘ë³µ ì œê±°
        seen_names = set()
        unique_startups = []
        
        for startup in self.ecosystem_data['startups']:
            name_normalized = startup['name'].lower().strip()
            if name_normalized not in seen_names:
                seen_names.add(name_normalized)
                unique_startups.append(startup)
        
        self.ecosystem_data['startups'] = unique_startups
        
        logger.info(f"ì¤‘ë³µ ì œê±° í›„ ìŠ¤íƒ€íŠ¸ì—… ìˆ˜: {len(unique_startups)}")
    
    def _add_statistics(self):
        """í†µê³„ ì •ë³´ ì¶”ê°€"""
        stats = {
            'total_startups': len(self.ecosystem_data['startups']),
            'total_investors': len(self.ecosystem_data['investors']),
            'total_accelerators': len(self.ecosystem_data['accelerators']),
            'total_coworking_spaces': len(self.ecosystem_data['coworking_spaces']),
            'total_events': len(self.ecosystem_data['events']),
            'total_entities': sum([
                len(self.ecosystem_data['startups']),
                len(self.ecosystem_data['investors']),
                len(self.ecosystem_data['accelerators']),
                len(self.ecosystem_data['coworking_spaces']),
                len(self.ecosystem_data['events'])
            ])
        }
        
        self.ecosystem_data['statistics'] = stats
        logger.info(f"í†µê³„ ì •ë³´ ì¶”ê°€ ì™„ë£Œ: ì´ {stats['total_entities']}ê°œ ì—”í‹°í‹°")
    
    def save_data(self, filename: str = None):
        """ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"ecosystem_data_{timestamp}.json"
        
        filepath = f"data/{filename}"
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(self.ecosystem_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            return None

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("ğŸš€ ìŠ¤íƒ€íŠ¸ì—… ìƒíƒœê³„ íƒìƒ‰ í¬ë¡¤ëŸ¬ ì‹œì‘")
    
    async with EcosystemCrawler() as crawler:
        try:
            # ì „ì²´ ì†ŒìŠ¤ì—ì„œ ë°ì´í„° í¬ë¡¤ë§
            ecosystem_data = await crawler.crawl_all_sources()
            
            # ë°ì´í„° ì €ì¥
            filename = crawler.save_data()
            
            if filename:
                logger.info(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ! ë°ì´í„° ì €ì¥ë¨: {filename}")
                
                # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
                stats = ecosystem_data.get('statistics', {})
                logger.info(f"ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½:")
                logger.info(f"   - ìŠ¤íƒ€íŠ¸ì—…: {stats.get('total_startups', 0)}ê°œ")
                logger.info(f"   - íˆ¬ìì: {stats.get('total_investors', 0)}ê°œ")
                logger.info(f"   - ì•¡ì…€ëŸ¬ë ˆì´í„°: {stats.get('total_accelerators', 0)}ê°œ")
                logger.info(f"   - ì½”ì›Œí‚¹ ìŠ¤í˜ì´ìŠ¤: {stats.get('total_coworking_spaces', 0)}ê°œ")
                logger.info(f"   - ì´ë²¤íŠ¸: {stats.get('total_events', 0)}ê°œ")
                logger.info(f"   - ì´ ì—”í‹°í‹°: {stats.get('total_entities', 0)}ê°œ")
            else:
                logger.error("âŒ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨")
                
        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    asyncio.run(main())
