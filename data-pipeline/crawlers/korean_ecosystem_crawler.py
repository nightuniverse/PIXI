#!/usr/bin/env python3
"""
í•œêµ­ ìŠ¤íƒ€íŠ¸ì—… ìƒíƒœê³„ íŠ¹í™” í¬ë¡¤ëŸ¬
í•œêµ­ì˜ ì£¼ìš” ìŠ¤íƒ€íŠ¸ì—… í”Œë«í¼ê³¼ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì—ì„œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

import asyncio
import json
import logging
import time
import random
from datetime import datetime
from typing import Dict, List, Optional, Any
from urllib.parse import urljoin, urlparse, quote
import re

import aiohttp
import pandas as pd
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('korean_ecosystem_crawler.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class KoreanEcosystemCrawler:
    def __init__(self):
        self.session = None
        self.browser = None
        self.page = None
        self.korean_ecosystem_data = {
            'startups': [],
            'investors': [],
            'accelerators': [],
            'coworking_spaces': [],
            'news': [],
            'crawled_at': datetime.now().isoformat()
        }
        
        # User-Agent ëª©ë¡
        self.user_agents = [
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]
        
        # í¬ë¡¤ë§ ì„¤ì •
        self.delay_range = (3, 6)  # í•œêµ­ ì‚¬ì´íŠ¸ëŠ” ë” ê¸´ ì§€ì—° ì‹œê°„
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(
            headless=True,
            args=['--no-sandbox', '--disable-dev-shm-usage', '--disable-blink-features=AutomationControlled']
        )
        self.page = await self.browser.new_page()
        
        # User-Agent ì„¤ì •
        await self.page.set_extra_http_headers({
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()
    
    async def random_delay(self, min_delay=None, max_delay=None):
        """ëœë¤ ì§€ì—°ìœ¼ë¡œ ì°¨ë‹¨ ë°©ì§€"""
        if min_delay is None:
            min_delay = self.delay_range[0]
        if max_delay is None:
            max_delay = self.delay_range[1]
        
        delay = random.uniform(min_delay, max_delay)
        await asyncio.sleep(delay)
    
    async def crawl_platum_startups(self, max_pages: int = 10) -> List[Dict]:
        """í”Œë˜í…€ì—ì„œ í•œêµ­ ìŠ¤íƒ€íŠ¸ì—… ì •ë³´ í¬ë¡¤ë§"""
        logger.info("í”Œë˜í…€ í•œêµ­ ìŠ¤íƒ€íŠ¸ì—… ì •ë³´ í¬ë¡¤ë§ ì‹œì‘")
        startups = []
        
        try:
            for page in range(1, max_pages + 1):
                url = f"https://platum.kr/startup/page/{page}/"
                logger.info(f"í”Œë˜í…€ í˜ì´ì§€ {page} í¬ë¡¤ë§: {url}")
                
                try:
                    await self.page.goto(url, wait_until='networkidle', timeout=30000)
                    await self.random_delay(2, 4)
                except Exception as e:
                    logger.warning(f"í˜ì´ì§€ {page} ë¡œë“œ ì‹¤íŒ¨: {e}")
                    continue
                
                # ê¸°ì‚¬ ëª©ë¡ ëŒ€ê¸° - ë‹¤ì–‘í•œ ì…€ë ‰í„° ì‹œë„
                articles = []
                selectors = ['article', '.post-item', '.entry', '.post', 'div[class*="post"]', 'div[class*="article"]']
                for selector in selectors:
                    try:
                        await self.page.wait_for_selector(selector, timeout=10000)
                        articles = await self.page.query_selector_all(selector)
                        if articles:
                            logger.info(f"ì…€ë ‰í„° '{selector}'ë¡œ {len(articles)}ê°œ ê¸°ì‚¬ ë°œê²¬")
                            break
                    except:
                        continue
                
                if not articles:
                    logger.warning(f"í˜ì´ì§€ {page}ì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    if page > 3:  # ì²˜ìŒ ëª‡ í˜ì´ì§€ëŠ” ê³„ì† ì‹œë„
                        break
                    continue
                
                for article in articles:
                    try:
                        # ì œëª©
                        title_elem = await article.query_selector('h2, h3, .post-title')
                        title = await title_elem.text_content() if title_elem else "No Title"
                        
                        # ë§í¬
                        link_elem = await article.query_selector('a')
                        link = await link_elem.get_attribute('href') if link_elem else ""
                        
                        # ìš”ì•½
                        excerpt_elem = await article.query_selector('p, .post-excerpt')
                        excerpt = await excerpt_elem.text_content() if excerpt_elem else ""
                        
                        # ì¹´í…Œê³ ë¦¬
                        category_elem = await article.query_selector('.category, .post-category')
                        category = await category_elem.text_content() if category_elem else "ìŠ¤íƒ€íŠ¸ì—…"
                        
                        startup_data = {
                            'name': title.strip()[:100],
                            'description': excerpt.strip()[:500],
                            'website': link if link.startswith('http') else f"https://platum.kr{link}",
                            'category': category.strip(),
                            'source': 'í”Œë˜í…€',
                            'type': 'startup',
                            'country': 'í•œêµ­',
                            'crawled_at': datetime.now().isoformat()
                        }
                        
                        startups.append(startup_data)
                        logger.info(f"í”Œë˜í…€ í¬ë¡¤ë§: {title[:50]}...")
                        
                    except Exception as e:
                        logger.error(f"ê¸°ì‚¬ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                        continue
                
                await self.random_delay(4, 7)  # í˜ì´ì§€ ê°„ ë” ê¸´ ì§€ì—°
                
        except Exception as e:
            logger.error(f"í”Œë˜í…€ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        
        logger.info(f"í”Œë˜í…€ í¬ë¡¤ë§ ì™„ë£Œ: {len(startups)}ê°œ ìŠ¤íƒ€íŠ¸ì—…")
        return startups
    
    async def crawl_techm_startups(self, max_pages: int = 3) -> List[Dict]:
        """í…Œí¬Mì—ì„œ í•œêµ­ ìŠ¤íƒ€íŠ¸ì—… ì •ë³´ í¬ë¡¤ë§"""
        logger.info("í…Œí¬M í•œêµ­ ìŠ¤íƒ€íŠ¸ì—… ì •ë³´ í¬ë¡¤ë§ ì‹œì‘")
        startups = []
        
        try:
            for page in range(1, max_pages + 1):
                url = f"https://www.techm.kr/news/articleList.html?page={page}&sc_section_code=S1N1&view_type=sm"
                logger.info(f"í…Œí¬M í˜ì´ì§€ {page} í¬ë¡¤ë§: {url}")
                
                await self.page.goto(url, wait_until='networkidle')
                await self.random_delay()
                
                # ê¸°ì‚¬ ëª©ë¡ ëŒ€ê¸°
                try:
                    await self.page.wait_for_selector('.list-block, .article-list', timeout=15000)
                except:
                    logger.warning(f"í˜ì´ì§€ {page}ì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    break
                
                articles = await self.page.query_selector_all('.list-block, .article-list')
                
                for article in articles:
                    try:
                        # ì œëª©
                        title_elem = await article.query_selector('h3, h4, .list-titles')
                        title = await title_elem.text_content() if title_elem else "No Title"
                        
                        # ë§í¬
                        link_elem = await article.query_selector('a')
                        link = await link_elem.get_attribute('href') if link_elem else ""
                        
                        # ìš”ì•½
                        excerpt_elem = await article.query_selector('p, .list-summary')
                        excerpt = await excerpt_elem.text_content() if excerpt_elem else ""
                        
                        # ì‘ì„±ì
                        author_elem = await article.query_selector('.byline, .writer')
                        author = await author_elem.text_content() if author_elem else ""
                        
                        startup_data = {
                            'name': title.strip()[:100],
                            'description': excerpt.strip()[:500],
                            'website': link if link.startswith('http') else f"https://www.techm.kr{link}",
                            'category': 'ìŠ¤íƒ€íŠ¸ì—…',
                            'author': author.strip(),
                            'source': 'í…Œí¬M',
                            'type': 'startup',
                            'country': 'í•œêµ­',
                            'crawled_at': datetime.now().isoformat()
                        }
                        
                        startups.append(startup_data)
                        logger.info(f"í…Œí¬M í¬ë¡¤ë§: {title[:50]}...")
                        
                    except Exception as e:
                        logger.error(f"ê¸°ì‚¬ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                        continue
                
                await self.random_delay(4, 7)
                
        except Exception as e:
            logger.error(f"í…Œí¬M í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        
        logger.info(f"í…Œí¬M í¬ë¡¤ë§ ì™„ë£Œ: {len(startups)}ê°œ ìŠ¤íƒ€íŠ¸ì—…")
        return startups
    
    async def crawl_startup_ranking(self, max_results: int = 50) -> List[Dict]:
        """ìŠ¤íƒ€íŠ¸ì—… ë­í‚¹ ì‚¬ì´íŠ¸ì—ì„œ ì •ë³´ í¬ë¡¤ë§"""
        logger.info("ìŠ¤íƒ€íŠ¸ì—… ë­í‚¹ ì •ë³´ í¬ë¡¤ë§ ì‹œì‘")
        startups = []
        
        # ìŠ¤íƒ€íŠ¸ì—… ë­í‚¹ ì‚¬ì´íŠ¸ë“¤
        ranking_sites = [
            "https://www.startupranking.com/country/kr",
            "https://www.startupblink.com/ecosystem/korea"
        ]
        
        for site_url in ranking_sites:
            try:
                logger.info(f"ë­í‚¹ ì‚¬ì´íŠ¸ í¬ë¡¤ë§: {site_url}")
                
                await self.page.goto(site_url, wait_until='networkidle')
                await self.random_delay()
                
                # ë­í‚¹ ëª©ë¡ ëŒ€ê¸°
                try:
                    await self.page.wait_for_selector('.startup-item, .ranking-item, .company-card', timeout=15000)
                except:
                    logger.warning(f"ë­í‚¹ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {site_url}")
                    continue
                
                startup_items = await self.page.query_selector_all('.startup-item, .ranking-item, .company-card')
                
                for i, item in enumerate(startup_items[:max_results//len(ranking_sites)]):
                    try:
                        # íšŒì‚¬ëª…
                        name_elem = await item.query_selector('h3, h4, .company-name')
                        name = await name_elem.text_content() if name_elem else "Unknown"
                        
                        # ì„¤ëª…
                        desc_elem = await item.query_selector('p, .description')
                        description = await desc_elem.text_content() if desc_elem else ""
                        
                        # ìˆœìœ„
                        rank_elem = await item.query_selector('.rank, .ranking')
                        rank = await rank_elem.text_content() if rank_elem else ""
                        
                        # ìœ„ì¹˜
                        location_elem = await item.query_selector('.location, .city')
                        location = await location_elem.text_content() if location_elem else "í•œêµ­"
                        
                        startup_data = {
                            'name': name.strip(),
                            'description': description.strip()[:500],
                            'location': location.strip(),
                            'ranking': rank.strip(),
                            'source': 'Startup Ranking',
                            'type': 'startup',
                            'country': 'í•œêµ­',
                            'crawled_at': datetime.now().isoformat()
                        }
                        
                        startups.append(startup_data)
                        logger.info(f"ë­í‚¹ í¬ë¡¤ë§: {name[:50]}... (ìˆœìœ„: {rank})")
                        
                    except Exception as e:
                        logger.error(f"ë­í‚¹ í•­ëª© í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                        continue
                
                await self.random_delay(3, 5)
                
            except Exception as e:
                logger.error(f"ë­í‚¹ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                continue
        
        logger.info(f"ë­í‚¹ í¬ë¡¤ë§ ì™„ë£Œ: {len(startups)}ê°œ ìŠ¤íƒ€íŠ¸ì—…")
        return startups
    
    async def crawl_korean_accelerators(self) -> List[Dict]:
        """í•œêµ­ ì•¡ì…€ëŸ¬ë ˆì´í„° ì •ë³´ í¬ë¡¤ë§"""
        logger.info("í•œêµ­ ì•¡ì…€ëŸ¬ë ˆì´í„° ì •ë³´ í¬ë¡¤ë§ ì‹œì‘")
        accelerators = []
        
        # í•œêµ­ ì£¼ìš” ì•¡ì…€ëŸ¬ë ˆì´í„° ì •ë³´ (í™•ì¥)
        korean_accelerators = [
            {
                "name": "ë”ë²¤ì²˜ìŠ¤",
                "description": "í•œêµ­ì˜ ëŒ€í‘œì ì¸ ë²¤ì²˜ìºí”¼íƒˆ ë° ì•¡ì…€ëŸ¬ë ˆì´í„°",
                "location": "ì„œìš¸",
                "website": "https://www.theventures.co.kr",
                "focus": "IT, ë°”ì´ì˜¤, ê²Œì„ ë“±",
                "source": "Manual Data"
            },
            {
                "name": "ìŠ¤ë§ˆì¼ê²Œì´íŠ¸ì¸ë² ìŠ¤íŠ¸ë¨¼íŠ¸",
                "description": "ê²Œì„ ë° IT ë¶„ì•¼ íˆ¬ì ì „ë¬¸ ì•¡ì…€ëŸ¬ë ˆì´í„°",
                "location": "ì„œìš¸",
                "website": "https://www.smilegate.com",
                "focus": "ê²Œì„, IT, ì—”í„°í…Œì¸ë¨¼íŠ¸",
                "source": "Manual Data"
            },
            {
                "name": "ë„¤ì´ë²„ D2SF",
                "description": "ë„¤ì´ë²„ì˜ ìŠ¤íƒ€íŠ¸ì—… ì§€ì› í”„ë¡œê·¸ë¨",
                "location": "ì„±ë‚¨",
                "website": "https://d2.naver.com",
                "focus": "AI, ë¹…ë°ì´í„°, ëª¨ë°”ì¼",
                "source": "Manual Data"
            },
            {
                "name": "ì¹´ì¹´ì˜¤ë²¤ì²˜ìŠ¤",
                "description": "ì¹´ì¹´ì˜¤ì˜ ë²¤ì²˜ìºí”¼íƒˆ ë° ì•¡ì…€ëŸ¬ë ˆì´í„°",
                "location": "ì œì£¼",
                "website": "https://ventures.kakao.com",
                "focus": "ëª¨ë°”ì¼, ì†Œì…œ, AI",
                "source": "Manual Data"
            },
            {
                "name": "LGë…¸íŠ¸",
                "description": "LGì˜ ìŠ¤íƒ€íŠ¸ì—… ì§€ì› í”„ë¡œê·¸ë¨",
                "location": "ì„œìš¸",
                "website": "https://www.lgnot.com",
                "focus": "IoT, AI, ë¡œë´‡",
                "source": "Manual Data"
            },
            {
                "name": "ìŠ¤íŒŒí¬ë©",
                "description": "ë„¤ì´ë²„ì˜ ìŠ¤íƒ€íŠ¸ì—… ì•¡ì…€ëŸ¬ë ˆì´í„°",
                "location": "ì„±ë‚¨",
                "website": "https://sparklab.co.kr",
                "focus": "AI, ë¹…ë°ì´í„°, ëª¨ë°”ì¼",
                "source": "Manual Data"
            },
            {
                "name": "ì¹´ì¹´ì˜¤ì¸ë² ìŠ¤íŠ¸ë¨¼íŠ¸",
                "description": "ì¹´ì¹´ì˜¤ì˜ íˆ¬ì ì „ë‹´ íšŒì‚¬",
                "location": "ì„œìš¸",
                "website": "https://www.kakao.com/investment",
                "focus": "ëª¨ë°”ì¼, ì†Œì…œ, AI, ì½˜í…ì¸ ",
                "source": "Manual Data"
            },
            {
                "name": "ë„¤ì´ë²„ D2SF",
                "description": "ë„¤ì´ë²„ì˜ ìŠ¤íƒ€íŠ¸ì—… ì§€ì› í”„ë¡œê·¸ë¨",
                "location": "ì„±ë‚¨",
                "website": "https://d2.naver.com",
                "focus": "AI, ë¹…ë°ì´í„°, ëª¨ë°”ì¼",
                "source": "Manual Data"
            },
            {
                "name": "ë²¤ì²˜ìŠ¤í€˜ì–´",
                "description": "ìŠ¤íƒ€íŠ¸ì—… ì•¡ì…€ëŸ¬ë ˆì´í„° ë° ì½”ì›Œí‚¹ ìŠ¤í˜ì´ìŠ¤",
                "location": "ì„œìš¸",
                "website": "https://venturesquare.net",
                "focus": "ìŠ¤íƒ€íŠ¸ì—…, ì°½ì—… ì§€ì›",
                "source": "Manual Data"
            },
            {
                "name": "í”ŒëŸ¬ìŠ¤ì—‘ìŠ¤",
                "description": "LGì˜ ìŠ¤íƒ€íŠ¸ì—… ì•¡ì…€ëŸ¬ë ˆì´í„°",
                "location": "ì„œìš¸",
                "website": "https://plusx.co.kr",
                "focus": "IoT, AI, ë¡œë´‡, ìŠ¤ë§ˆíŠ¸í™ˆ",
                "source": "Manual Data"
            }
        ]
        
        for acc in korean_accelerators:
            acc['type'] = 'accelerator'
            acc['country'] = 'í•œêµ­'
            acc['crawled_at'] = datetime.now().isoformat()
            accelerators.append(acc)
        
        logger.info(f"í•œêµ­ ì•¡ì…€ëŸ¬ë ˆì´í„° ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {len(accelerators)}ê°œ")
        return accelerators
    
    async def crawl_korean_coworking_spaces(self) -> List[Dict]:
        """í•œêµ­ ì½”ì›Œí‚¹ ìŠ¤í˜ì´ìŠ¤ ì •ë³´ í¬ë¡¤ë§"""
        logger.info("í•œêµ­ ì½”ì›Œí‚¹ ìŠ¤í˜ì´ìŠ¤ ì •ë³´ í¬ë¡¤ë§ ì‹œì‘")
        coworking_spaces = []
        
        # í•œêµ­ ì£¼ìš” ì½”ì›Œí‚¹ ìŠ¤í˜ì´ìŠ¤ ì •ë³´
        korean_coworking = [
            {
                "name": "ìœ„ì›Œí¬",
                "description": "ê¸€ë¡œë²Œ ì½”ì›Œí‚¹ ìŠ¤í˜ì´ìŠ¤ ì²´ì¸",
                "location": "ì„œìš¸ ê°•ë‚¨êµ¬",
                "website": "https://www.wework.com",
                "focus": "ìŠ¤íƒ€íŠ¸ì—…, í”„ë¦¬ëœì„œ, ì¤‘ì†Œê¸°ì—…",
                "source": "Manual Data"
            },
            {
                "name": "ìŠ¤íŒŒí¬í”ŒëŸ¬ìŠ¤",
                "description": "í•œêµ­ì˜ ëŒ€í‘œì ì¸ ì½”ì›Œí‚¹ ìŠ¤í˜ì´ìŠ¤",
                "location": "ì„œìš¸ ê°•ë‚¨êµ¬",
                "website": "https://www.sparkplus.co.kr",
                "focus": "ìŠ¤íƒ€íŠ¸ì—…, íˆ¬ìì‚¬, ì•¡ì…€ëŸ¬ë ˆì´í„°",
                "source": "Manual Data"
            },
            {
                "name": "ë§ˆë£¨180",
                "description": "ì„œìš¸ ì°½ì—…í—ˆë¸Œì˜ ëŒ€í‘œì ì¸ ì½”ì›Œí‚¹ ìŠ¤í˜ì´ìŠ¤",
                "location": "ì„œìš¸ ë§ˆí¬êµ¬",
                "website": "https://maru180.com",
                "focus": "ì°½ì—…, ì†Œì…œë²¤ì²˜, ì‚¬íšŒí˜ì‹ ",
                "source": "Manual Data"
            },
            {
                "name": "íŒêµí…Œí¬ë…¸ë°¸ë¦¬",
                "description": "í•œêµ­ì˜ ì‹¤ë¦¬ì½˜ë°¸ë¦¬ë¼ ë¶ˆë¦¬ëŠ” IT í´ëŸ¬ìŠ¤í„°",
                "location": "ê²½ê¸°ë„ ì„±ë‚¨ì‹œ",
                "website": "https://www.pangyo.or.kr",
                "focus": "IT, ë°”ì´ì˜¤, ë‚˜ë…¸ê¸°ìˆ ",
                "source": "Manual Data"
            }
        ]
        
        for space in korean_coworking:
            space['type'] = 'coworking_space'
            space['country'] = 'í•œêµ­'
            space['crawled_at'] = datetime.now().isoformat()
            coworking_spaces.append(space)
        
        logger.info(f"í•œêµ­ ì½”ì›Œí‚¹ ìŠ¤í˜ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {len(coworking_spaces)}ê°œ")
        return coworking_spaces
    
    async def crawl_manual_startups(self) -> List[Dict]:
        """ìˆ˜ë™ìœ¼ë¡œ ìˆ˜ì§‘í•œ í•œêµ­ ì£¼ìš” ìŠ¤íƒ€íŠ¸ì—… ë°ì´í„°"""
        logger.info("ìˆ˜ë™ í•œêµ­ ìŠ¤íƒ€íŠ¸ì—… ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        
        manual_startups = [
            {
                "name": "ì¹´ì¹´ì˜¤",
                "description": "ëª¨ë°”ì¼ í”Œë«í¼ ë° ì„œë¹„ìŠ¤ ê¸°ì—…",
                "location": "ì œì£¼",
                "website": "https://www.kakaocorp.com",
                "category": "í”Œë«í¼",
                "source": "Manual Data"
            },
            {
                "name": "ë„¤ì´ë²„",
                "description": "ì¸í„°ë„· í¬í„¸ ë° IT ì„œë¹„ìŠ¤ ê¸°ì—…",
                "location": "ì„±ë‚¨",
                "website": "https://www.naver.com",
                "category": "í”Œë«í¼",
                "source": "Manual Data"
            },
            {
                "name": "ì¿ íŒ¡",
                "description": "ì´ì»¤ë¨¸ìŠ¤ í”Œë«í¼",
                "location": "ì„œìš¸",
                "website": "https://www.coupang.com",
                "category": "ì´ì»¤ë¨¸ìŠ¤",
                "source": "Manual Data"
            },
            {
                "name": "ë°°ë‹¬ì˜ë¯¼ì¡±",
                "description": "ë°°ë‹¬ ì£¼ë¬¸ í”Œë«í¼",
                "location": "ì„œìš¸",
                "website": "https://www.baemin.com",
                "category": "ë°°ë‹¬",
                "source": "Manual Data"
            },
            {
                "name": "í† ìŠ¤",
                "description": "í•€í…Œí¬ ì„œë¹„ìŠ¤",
                "location": "ì„œìš¸",
                "website": "https://www.toss.im",
                "category": "í•€í…Œí¬",
                "source": "Manual Data"
            },
            {
                "name": "ë‹¹ê·¼ë§ˆì¼“",
                "description": "ì¤‘ê³ ê±°ë˜ í”Œë«í¼",
                "location": "ì„œìš¸",
                "website": "https://www.daangn.com",
                "category": "ì¤‘ê³ ê±°ë˜",
                "source": "Manual Data"
            },
            {
                "name": "ì•¼ë†€ì",
                "description": "ì—¬í–‰ ë° ìˆ™ë°• ì˜ˆì•½ í”Œë«í¼",
                "location": "ì„œìš¸",
                "website": "https://www.yanolja.com",
                "category": "ì—¬í–‰",
                "source": "Manual Data"
            },
            {
                "name": "ë¬´ì‹ ì‚¬",
                "description": "íŒ¨ì…˜ ì´ì»¤ë¨¸ìŠ¤ í”Œë«í¼",
                "location": "ì„œìš¸",
                "website": "https://www.musinsa.com",
                "category": "íŒ¨ì…˜",
                "source": "Manual Data"
            },
            {
                "name": "ë¼ì¸",
                "description": "ë©”ì‹ ì € ë° í”Œë«í¼ ì„œë¹„ìŠ¤",
                "location": "ì„œìš¸",
                "website": "https://line.me",
                "category": "ë©”ì‹ ì €",
                "source": "Manual Data"
            },
            {
                "name": "ìŠ¤íƒ€ì¼ì‰ì–´",
                "description": "íŒ¨ì…˜ ë Œíƒˆ í”Œë«í¼",
                "location": "ì„œìš¸",
                "website": "https://www.styleshare.kr",
                "category": "íŒ¨ì…˜",
                "source": "Manual Data"
            }
        ]
        
        for startup in manual_startups:
            startup['type'] = 'startup'
            startup['country'] = 'í•œêµ­'
            startup['crawled_at'] = datetime.now().isoformat()
        
        logger.info(f"ìˆ˜ë™ í•œêµ­ ìŠ¤íƒ€íŠ¸ì—… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(manual_startups)}ê°œ")
        return manual_startups
    
    async def crawl_all_korean_sources(self) -> Dict[str, Any]:
        """ëª¨ë“  í•œêµ­ ì†ŒìŠ¤ì—ì„œ ë°ì´í„° í¬ë¡¤ë§"""
        logger.info("í•œêµ­ ìƒíƒœê³„ ì „ì²´ ë°ì´í„° í¬ë¡¤ë§ ì‹œì‘")
        
        # 1. í”Œë˜í…€ ìŠ¤íƒ€íŠ¸ì—… ì •ë³´
        platum_startups = await self.crawl_platum_startups(max_pages=10)
        self.korean_ecosystem_data['startups'].extend(platum_startups)
        
        # 2. í…Œí¬M ìŠ¤íƒ€íŠ¸ì—… ì •ë³´
        techm_startups = await self.crawl_techm_startups()
        self.korean_ecosystem_data['startups'].extend(techm_startups)
        
        # 3. ìŠ¤íƒ€íŠ¸ì—… ë­í‚¹ ì •ë³´
        ranking_startups = await self.crawl_startup_ranking()
        self.korean_ecosystem_data['startups'].extend(ranking_startups)
        
        # 4. ìˆ˜ë™ ìŠ¤íƒ€íŠ¸ì—… ë°ì´í„°
        manual_startups = await self.crawl_manual_startups()
        self.korean_ecosystem_data['startups'].extend(manual_startups)
        
        # 5. í•œêµ­ ì•¡ì…€ëŸ¬ë ˆì´í„° ì •ë³´
        korean_accelerators = await self.crawl_korean_accelerators()
        self.korean_ecosystem_data['accelerators'].extend(korean_accelerators)
        
        # 6. í•œêµ­ ì½”ì›Œí‚¹ ìŠ¤í˜ì´ìŠ¤ ì •ë³´
        korean_coworking = await self.crawl_korean_coworking_spaces()
        self.korean_ecosystem_data['coworking_spaces'].extend(korean_coworking)
        
        # ì¤‘ë³µ ì œê±°
        self._remove_duplicates()
        
        # í†µê³„ ì •ë³´ ì¶”ê°€
        self._add_statistics()
        
        logger.info("í•œêµ­ ìƒíƒœê³„ ì „ì²´ ë°ì´í„° í¬ë¡¤ë§ ì™„ë£Œ")
        return self.korean_ecosystem_data
    
    def _remove_duplicates(self):
        """ì¤‘ë³µ ë°ì´í„° ì œê±°"""
        logger.info("ì¤‘ë³µ ë°ì´í„° ì œê±° ì¤‘...")
        
        # íšŒì‚¬ëª… ê¸°ë°˜ ì¤‘ë³µ ì œê±°
        seen_names = set()
        unique_startups = []
        
        for startup in self.korean_ecosystem_data['startups']:
            name_normalized = startup['name'].lower().strip()
            if name_normalized not in seen_names:
                seen_names.add(name_normalized)
                unique_startups.append(startup)
        
        self.korean_ecosystem_data['startups'] = unique_startups
        
        logger.info(f"ì¤‘ë³µ ì œê±° í›„ ìŠ¤íƒ€íŠ¸ì—… ìˆ˜: {len(unique_startups)}")
    
    def _add_statistics(self):
        """í†µê³„ ì •ë³´ ì¶”ê°€"""
        stats = {
            'total_startups': len(self.korean_ecosystem_data['startups']),
            'total_investors': len(self.korean_ecosystem_data['investors']),
            'total_accelerators': len(self.korean_ecosystem_data['accelerators']),
            'total_coworking_spaces': len(self.korean_ecosystem_data['coworking_spaces']),
            'total_news': len(self.korean_ecosystem_data['news']),
            'total_entities': sum([
                len(self.korean_ecosystem_data['startups']),
                len(self.korean_ecosystem_data['investors']),
                len(self.korean_ecosystem_data['accelerators']),
                len(self.korean_ecosystem_data['coworking_spaces']),
                len(self.korean_ecosystem_data['news'])
            ])
        }
        
        self.korean_ecosystem_data['statistics'] = stats
        logger.info(f"í†µê³„ ì •ë³´ ì¶”ê°€ ì™„ë£Œ: ì´ {stats['total_entities']}ê°œ ì—”í‹°í‹°")
    
    def save_data(self, filename: str = None):
        """ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"korean_ecosystem_data_{timestamp}.json"
        
        filepath = f"data/{filename}"
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(self.korean_ecosystem_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            return None

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("ğŸ‡°ğŸ‡· í•œêµ­ ìŠ¤íƒ€íŠ¸ì—… ìƒíƒœê³„ íƒìƒ‰ í¬ë¡¤ëŸ¬ ì‹œì‘")
    
    async with KoreanEcosystemCrawler() as crawler:
        try:
            # ì „ì²´ í•œêµ­ ì†ŒìŠ¤ì—ì„œ ë°ì´í„° í¬ë¡¤ë§
            korean_ecosystem_data = await crawler.crawl_all_korean_sources()
            
            # ë°ì´í„° ì €ì¥
            filename = crawler.save_data()
            
            if filename:
                logger.info(f"âœ… í•œêµ­ ìƒíƒœê³„ í¬ë¡¤ë§ ì™„ë£Œ! ë°ì´í„° ì €ì¥ë¨: {filename}")
                
                # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
                stats = korean_ecosystem_data.get('statistics', {})
                logger.info(f"ğŸ“Š í•œêµ­ ìƒíƒœê³„ í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½:")
                logger.info(f"   - ìŠ¤íƒ€íŠ¸ì—…: {stats.get('total_startups', 0)}ê°œ")
                logger.info(f"   - íˆ¬ìì: {stats.get('total_investors', 0)}ê°œ")
                logger.info(f"   - ì•¡ì…€ëŸ¬ë ˆì´í„°: {stats.get('total_accelerators', 0)}ê°œ")
                logger.info(f"   - ì½”ì›Œí‚¹ ìŠ¤í˜ì´ìŠ¤: {stats.get('total_coworking_spaces', 0)}ê°œ")
                logger.info(f"   - ë‰´ìŠ¤: {stats.get('total_news', 0)}ê°œ")
                logger.info(f"   - ì´ ì—”í‹°í‹°: {stats.get('total_entities', 0)}ê°œ")
            else:
                logger.error("âŒ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨")
                
        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    asyncio.run(main())
