#!/usr/bin/env python3
"""
ê³ ê¸‰ ìŠ¤íƒ€íŠ¸ì—… ì •ë³´ í¬ë¡¤ë§ ì‹œìŠ¤í…œ
ì—¬ëŸ¬ ì‚¬ì´íŠ¸ì—ì„œ í’ë¶€í•œ ìŠ¤íƒ€íŠ¸ì—… ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

import asyncio
import json
import logging
import time
import random
from datetime import datetime
from typing import Dict, List, Optional
from urllib.parse import urljoin, urlparse, quote

import aiohttp
import pandas as pd
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('advanced_crawler.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class AdvancedStartupCrawler:
    def __init__(self):
        self.session = None
        self.browser = None
        self.page = None
        self.startups_data = []
        
        # User-Agent ëª©ë¡ (ì°¨ë‹¨ ë°©ì§€)
        self.user_agents = [
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        ]
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(
            headless=True,
            args=['--no-sandbox', '--disable-dev-shm-usage']
        )
        self.page = await self.browser.new_page()
        
        # User-Agent ì„¤ì •
        await self.page.set_extra_http_headers({
            'User-Agent': random.choice(self.user_agents)
        })
        
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()
    
    async def random_delay(self, min_delay=1, max_delay=3):
        """ëœë¤ ì§€ì—°ìœ¼ë¡œ ì°¨ë‹¨ ë°©ì§€"""
        delay = random.uniform(min_delay, max_delay)
        await asyncio.sleep(delay)
    
    async def crawl_crunchbase_advanced(self, search_queries: List[str] = None, max_results: int = 30) -> List[Dict]:
        """Crunchbaseì—ì„œ ê³ ê¸‰ ìŠ¤íƒ€íŠ¸ì—… ì •ë³´ í¬ë¡¤ë§"""
        if search_queries is None:
            search_queries = ["AI startups", "Fintech", "HealthTech", "EdTech"]
        
        logger.info(f"Crunchbase ê³ ê¸‰ í¬ë¡¤ë§ ì‹œì‘: {search_queries}")
        all_startups = []
        
        for query in search_queries:
            try:
                logger.info(f"'{query}' ê²€ìƒ‰ ì¤‘...")
                
                # ê²€ìƒ‰ URL êµ¬ì„±
                search_url = f"https://www.crunchbase.com/search/organizations?query={quote(query)}"
                await self.page.goto(search_url, wait_until='networkidle')
                await self.random_delay()
                
                # ê²€ìƒ‰ ê²°ê³¼ ëŒ€ê¸°
                try:
                    await self.page.wait_for_selector('.result-card, .search-result', timeout=15000)
                except:
                    logger.warning(f"'{query}' ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    continue
                
                results = await self.page.query_selector_all('.result-card, .search-result')
                logger.info(f"'{query}'ì—ì„œ {len(results)}ê°œ ê²°ê³¼ ë°œê²¬")
                
                for i, result in enumerate(results[:max_results//len(search_queries)]):
                    try:
                        # íšŒì‚¬ëª…
                        name_elem = await result.query_selector('.result-card__title, .search-result__title')
                        name = await name_elem.text_content() if name_elem else "Unknown"
                        
                        # ì„¤ëª…
                        desc_elem = await result.query_selector('.result-card__description, .search-result__description')
                        description = await desc_elem.text_content() if desc_elem else ""
                        
                        # ìœ„ì¹˜
                        location_elem = await result.query_selector('.result-card__location, .search-result__location')
                        location = await location_elem.text_content() if location_elem else ""
                        
                        # í€ë”© ì •ë³´
                        funding_elem = await result.query_selector('.result-card__funding, .search-result__funding')
                        funding = await funding_elem.text_content() if funding_elem else ""
                        
                        # ì‚°ì—… ë¶„ì•¼
                        industry_elem = await result.query_selector('.result-card__industry, .search-result__industry')
                        industry = await industry_elem.text_content() if industry_elem else query
                        
                        startup_data = {
                            'name': name.strip(),
                            'description': description.strip(),
                            'location': location.strip(),
                            'funding': funding.strip(),
                            'industry': industry.strip(),
                            'search_query': query,
                            'source': 'Crunchbase',
                            'crawled_at': datetime.now().isoformat()
                        }
                        
                        all_startups.append(startup_data)
                        logger.info(f"âœ… {name[:30]}... ({query})")
                        
                    except Exception as e:
                        logger.error(f"ê°œë³„ ê²°ê³¼ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                        continue
                    
                    await self.random_delay(0.5, 1.5)
                
                await self.random_delay(2, 4)
                
            except Exception as e:
                logger.error(f"'{query}' í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                continue
        
        logger.info(f"Crunchbase ê³ ê¸‰ í¬ë¡¤ë§ ì™„ë£Œ: {len(all_startups)}ê°œ ìŠ¤íƒ€íŠ¸ì—…")
        return all_startups
    
    async def crawl_angel_list_advanced(self, search_queries: List[str] = None, max_results: int = 30) -> List[Dict]:
        """AngelListì—ì„œ ê³ ê¸‰ ìŠ¤íƒ€íŠ¸ì—… ì •ë³´ í¬ë¡¤ë§"""
        if search_queries is None:
            search_queries = ["AI", "Fintech", "Health", "Education"]
        
        logger.info(f"AngelList ê³ ê¸‰ í¬ë¡¤ë§ ì‹œì‘: {search_queries}")
        all_startups = []
        
        for query in search_queries:
            try:
                logger.info(f"'{query}' ê²€ìƒ‰ ì¤‘...")
                
                # ê²€ìƒ‰ URL êµ¬ì„±
                search_url = f"https://angel.co/companies?keywords={quote(query)}"
                await self.page.goto(search_url, wait_until='networkidle')
                await self.random_delay()
                
                # ê²€ìƒ‰ ê²°ê³¼ ëŒ€ê¸°
                try:
                    await self.page.wait_for_selector('.company, .company-card', timeout=15000)
                except:
                    logger.warning(f"'{query}' ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    continue
                
                results = await self.page.query_selector_all('.company, .company-card')
                logger.info(f"'{query}'ì—ì„œ {len(results)}ê°œ ê²°ê³¼ ë°œê²¬")
                
                for i, result in enumerate(results[:max_results//len(search_queries)]):
                    try:
                        # íšŒì‚¬ëª…
                        name_elem = await result.query_selector('.company-name, .company-card__name')
                        name = await name_elem.text_content() if name_elem else "Unknown"
                        
                        # ì„¤ëª…
                        desc_elem = await result.query_selector('.company-description, .company-card__description')
                        description = await desc_elem.text_content() if desc_elem else ""
                        
                        # ìœ„ì¹˜
                        location_elem = await result.query_selector('.company-location, .company-card__location')
                        location = await location_elem.text_content() if location_elem else ""
                        
                        # íƒœê·¸
                        tags_elem = await result.query_selector_all('.company-tags .tag, .company-card__tags .tag')
                        tags = []
                        for tag in tags_elem:
                            tag_text = await tag.text_content()
                            if tag_text:
                                tags.append(tag_text.strip())
                        
                        # í€ë”© ë‹¨ê³„
                        stage_elem = await result.query_selector('.company-stage, .company-card__stage')
                        stage = await stage_elem.text_content() if stage_elem else ""
                        
                        startup_data = {
                            'name': name.strip(),
                            'description': description.strip(),
                            'location': location.strip(),
                            'tags': tags,
                            'stage': stage.strip(),
                            'search_query': query,
                            'source': 'AngelList',
                            'crawled_at': datetime.now().isoformat()
                        }
                        
                        all_startups.append(startup_data)
                        logger.info(f"âœ… {name[:30]}... ({query})")
                        
                    except Exception as e:
                        logger.error(f"ê°œë³„ ê²°ê³¼ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                        continue
                    
                    await self.random_delay(0.5, 1.5)
                
                await self.random_delay(2, 4)
                
            except Exception as e:
                logger.error(f"'{query}' í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                continue
        
        logger.info(f"AngelList ê³ ê¸‰ í¬ë¡¤ë§ ì™„ë£Œ: {len(all_startups)}ê°œ ìŠ¤íƒ€íŠ¸ì—…")
        return all_startups
    
    async def crawl_startup_blink_advanced(self, max_results: int = 50) -> List[Dict]:
        """StartupBlinkì—ì„œ ê¸€ë¡œë²Œ ìŠ¤íƒ€íŠ¸ì—… ë­í‚¹ í¬ë¡¤ë§"""
        logger.info("StartupBlink ê³ ê¸‰ í¬ë¡¤ë§ ì‹œì‘")
        
        try:
            # StartupBlink ë©”ì¸ í˜ì´ì§€ë¡œ ì´ë™
            await self.page.goto("https://www.startupblink.com/", wait_until='networkidle')
            await self.random_delay()
            
            # ê¸€ë¡œë²Œ ë­í‚¹ ì„¹ì…˜ ëŒ€ê¸°
            try:
                await self.page.wait_for_selector('.ranking-item, .startup-item', timeout=15000)
            except:
                logger.warning("StartupBlink ë­í‚¹ ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return []
            
            results = await self.page.query_selector_all('.ranking-item, .startup-item')
            logger.info(f"StartupBlinkì—ì„œ {len(results)}ê°œ ê²°ê³¼ ë°œê²¬")
            
            startups = []
            for i, result in enumerate(results[:max_results]):
                try:
                    # íšŒì‚¬ëª…
                    name_elem = await result.query_selector('.company-name, .startup-name')
                    name = await name_elem.text_content() if name_elem else "Unknown"
                    
                    # ë­í‚¹
                    rank_elem = await result.query_selector('.ranking-number, .startup-rank')
                    rank = await rank_elem.text_content() if rank_elem else ""
                    
                    # ìœ„ì¹˜
                    location_elem = await result.query_selector('.company-location, .startup-location')
                    location = await location_elem.text_content() if location_elem else ""
                    
                    # ì ìˆ˜
                    score_elem = await result.query_selector('.company-score, .startup-score')
                    score = await score_elem.text_content() if score_elem else ""
                    
                    # ì¹´í…Œê³ ë¦¬
                    category_elem = await result.query_selector('.company-category, .startup-category')
                    category = await category_elem.text_content() if category_elem else ""
                    
                    startup_data = {
                        'name': name.strip(),
                        'rank': rank.strip(),
                        'location': location.strip(),
                        'score': score.strip(),
                        'category': category.strip(),
                        'source': 'StartupBlink',
                        'crawled_at': datetime.now().isoformat()
                    }
                    
                    startups.append(startup_data)
                    logger.info(f"âœ… {name[:30]}... (ë­í‚¹: {rank})")
                    
                except Exception as e:
                    logger.error(f"ê°œë³„ ê²°ê³¼ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                    continue
                
                await self.random_delay(0.5, 1.5)
            
            logger.info(f"StartupBlink ê³ ê¸‰ í¬ë¡¤ë§ ì™„ë£Œ: {len(startups)}ê°œ ìŠ¤íƒ€íŠ¸ì—…")
            return startups
            
        except Exception as e:
            logger.error(f"StartupBlink ê³ ê¸‰ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return []
    
    async def crawl_techcrunch_advanced(self, max_results: int = 40) -> List[Dict]:
        """TechCrunchì—ì„œ ìŠ¤íƒ€íŠ¸ì—… ë‰´ìŠ¤ í¬ë¡¤ë§"""
        logger.info("TechCrunch ê³ ê¸‰ í¬ë¡¤ë§ ì‹œì‘")
        
        try:
            # TechCrunch ìŠ¤íƒ€íŠ¸ì—… ì„¹ì…˜
            await self.page.goto("https://techcrunch.com/category/startups/", wait_until='networkidle')
            await self.random_delay()
            
            # ê¸°ì‚¬ ëŒ€ê¸°
            try:
                await self.page.wait_for_selector('article, .post-block', timeout=15000)
            except:
                logger.warning("TechCrunch ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return []
            
            articles = await self.page.query_selector_all('article, .post-block')
            logger.info(f"TechCrunchì—ì„œ {len(articles)}ê°œ ê¸°ì‚¬ ë°œê²¬")
            
            startups = []
            for i, article in enumerate(articles[:max_results]):
                try:
                    # ì œëª©
                    title_elem = await article.query_selector('h2, .post-block__title, .post-title')
                    title = await title_elem.text_content() if title_elem else "No Title"
                    
                    # ë§í¬
                    link_elem = await article.query_selector('a[href*="/"]')
                    link = await link_elem.get_attribute('href') if link_elem else ""
                    if link and not link.startswith('http'):
                        link = f"https://techcrunch.com{link}"
                    
                    # ìš”ì•½
                    excerpt_elem = await article.query_selector('.post-block__content, .post-excerpt, p')
                    excerpt = await excerpt_elem.text_content() if excerpt_elem else ""
                    
                    # ì‘ì„±ì
                    author_elem = await article.query_selector('.river-byline__authors, .post-author, .author')
                    author = await author_elem.text_content() if author_elem else "Unknown"
                    
                    # ì‹œê°„
                    time_elem = await article.query_selector('time, .post-date, .date')
                    publish_time = await time_elem.text_content() if time_elem else ""
                    
                    startup_data = {
                        'title': title.strip(),
                        'link': link,
                        'excerpt': excerpt.strip(),
                        'author': author.strip(),
                        'publish_time': publish_time.strip(),
                        'source': 'TechCrunch',
                        'crawled_at': datetime.now().isoformat()
                    }
                    
                    startups.append(startup_data)
                    logger.info(f"âœ… {title[:40]}...")
                    
                except Exception as e:
                    logger.error(f"ê°œë³„ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                    continue
                
                await self.random_delay(0.5, 1.5)
            
            logger.info(f"TechCrunch ê³ ê¸‰ í¬ë¡¤ë§ ì™„ë£Œ: {len(startups)}ê°œ ê¸°ì‚¬")
            return startups
            
        except Exception as e:
            logger.error(f"TechCrunch ê³ ê¸‰ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return []
    
    async def crawl_startup_compass(self, max_results: int = 30) -> List[Dict]:
        """Startup Compassì—ì„œ ìŠ¤íƒ€íŠ¸ì—… ì •ë³´ í¬ë¡¤ë§"""
        logger.info("Startup Compass í¬ë¡¤ë§ ì‹œì‘")
        
        try:
            # Startup Compass ë©”ì¸ í˜ì´ì§€
            await self.page.goto("https://startupcompass.co/", wait_until='networkidle')
            await self.random_delay()
            
            # ìŠ¤íƒ€íŠ¸ì—… ëª©ë¡ ëŒ€ê¸°
            try:
                await self.page.wait_for_selector('.startup-item, .company-card', timeout=15000)
            except:
                logger.warning("Startup Compass ìŠ¤íƒ€íŠ¸ì—… ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return []
            
            results = await self.page.query_selector_all('.startup-item, .company-card')
            logger.info(f"Startup Compassì—ì„œ {len(results)}ê°œ ê²°ê³¼ ë°œê²¬")
            
            startups = []
            for i, result in enumerate(results[:max_results]):
                try:
                    # íšŒì‚¬ëª…
                    name_elem = await result.query_selector('.startup-name, .company-name')
                    name = await name_elem.text_content() if name_elem else "Unknown"
                    
                    # ì„¤ëª…
                    desc_elem = await result.query_selector('.startup-description, .company-description')
                    description = await desc_elem.text_content() if desc_elem else ""
                    
                    # ìœ„ì¹˜
                    location_elem = await result.query_selector('.startup-location, .company-location')
                    location = await location_elem.text_content() if location_elem else ""
                    
                    # ì‚°ì—…
                    industry_elem = await result.query_selector('.startup-industry, .company-industry')
                    industry = await industry_elem.text_content() if industry_elem else ""
                    
                    startup_data = {
                        'name': name.strip(),
                        'description': description.strip(),
                        'location': location.strip(),
                        'industry': industry.strip(),
                        'source': 'Startup Compass',
                        'crawled_at': datetime.now().isoformat()
                    }
                    
                    startups.append(startup_data)
                    logger.info(f"âœ… {name[:30]}...")
                    
                except Exception as e:
                    logger.error(f"ê°œë³„ ê²°ê³¼ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                    continue
                
                await self.random_delay(0.5, 1.5)
            
            logger.info(f"Startup Compass í¬ë¡¤ë§ ì™„ë£Œ: {len(startups)}ê°œ ìŠ¤íƒ€íŠ¸ì—…")
            return startups
            
        except Exception as e:
            logger.error(f"Startup Compass í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return []
    
    async def save_data(self, data: List[Dict], filename: str):
        """í¬ë¡¤ë§í•œ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        filepath = f"data/{filename}"
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filepath}")
        
        # CSVë¡œë„ ì €ì¥
        df = pd.DataFrame(data)
        csv_filepath = filepath.replace('.json', '.csv')
        df.to_csv(csv_filepath, index=False, encoding='utf-8')
        logger.info(f"CSV ì €ì¥ ì™„ë£Œ: {csv_filepath}")
    
    async def run_advanced_crawling(self):
        """ì „ì²´ ê³ ê¸‰ í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        logger.info("ê³ ê¸‰ ìŠ¤íƒ€íŠ¸ì—… í¬ë¡¤ë§ ì‹œì‘")
        
        all_startups = []
        
        # 1. Crunchbase ê³ ê¸‰ í¬ë¡¤ë§
        logger.info("=" * 50)
        crunchbase_data = await self.crawl_crunchbase_advanced(
            ["AI startups", "Fintech", "HealthTech", "EdTech", "SaaS"], 
            40
        )
        all_startups.extend(crunchbase_data)
        await self.save_data(crunchbase_data, "crunchbase_advanced.json")
        
        # 2. AngelList ê³ ê¸‰ í¬ë¡¤ë§
        logger.info("=" * 50)
        angel_list_data = await self.crawl_angel_list_advanced(
            ["AI", "Fintech", "Health", "Education", "Enterprise"], 
            40
        )
        all_startups.extend(angel_list_data)
        await self.save_data(angel_list_data, "angel_list_advanced.json")
        
        # 3. StartupBlink ê³ ê¸‰ í¬ë¡¤ë§
        logger.info("=" * 50)
        startup_blink_data = await self.crawl_startup_blink_advanced(40)
        all_startups.extend(startup_blink_data)
        await self.save_data(startup_blink_data, "startup_blink_advanced.json")
        
        # 4. TechCrunch ê³ ê¸‰ í¬ë¡¤ë§
        logger.info("=" * 50)
        techcrunch_data = await self.crawl_techcrunch_advanced(40)
        all_startups.extend(techcrunch_data)
        await self.save_data(techcrunch_data, "techcrunch_advanced.json")
        
        # 5. Startup Compass í¬ë¡¤ë§
        logger.info("=" * 50)
        startup_compass_data = await self.crawl_startup_compass(30)
        all_startups.extend(startup_compass_data)
        await self.save_data(startup_compass_data, "startup_compass.json")
        
        # ì „ì²´ ë°ì´í„° ì €ì¥
        await self.save_data(all_startups, "all_startups_advanced.json")
        
        # í†µê³„ ì •ë³´
        sources = {}
        for startup in all_startups:
            source = startup.get('source', 'Unknown')
            sources[source] = sources.get(source, 0) + 1
        
        logger.info("=" * 50)
        logger.info(f"ğŸ¯ ì „ì²´ ê³ ê¸‰ í¬ë¡¤ë§ ì™„ë£Œ: {len(all_startups)}ê°œ ìŠ¤íƒ€íŠ¸ì—…")
        logger.info("ğŸ“Š ì†ŒìŠ¤ë³„ í†µê³„:")
        for source, count in sources.items():
            logger.info(f"   {source}: {count}ê°œ")
        
        return all_startups

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    async with AdvancedStartupCrawler() as crawler:
        await crawler.run_advanced_crawling()

if __name__ == "__main__":
    asyncio.run(main())
