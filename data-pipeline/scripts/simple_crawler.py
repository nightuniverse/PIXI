#!/usr/bin/env python3
"""
ê°„ë‹¨í•œ ìŠ¤íƒ€íŠ¸ì—… ì •ë³´ í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸ (í…ŒìŠ¤íŠ¸ìš©)
"""

import requests
from bs4 import BeautifulSoup
import json
import time
from datetime import datetime

def crawl_startup_news():
    """TechCrunchì—ì„œ ìŠ¤íƒ€íŠ¸ì—… ë‰´ìŠ¤ í¬ë¡¤ë§"""
    print("ğŸš€ TechCrunchì—ì„œ ìŠ¤íƒ€íŠ¸ì—… ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘...")
    
    try:
        # TechCrunch ìŠ¤íƒ€íŠ¸ì—… ì„¹ì…˜
        url = "https://techcrunch.com/category/startups/"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        startups = []
        articles = soup.find_all('article', class_='post-block')
        
        for i, article in enumerate(articles[:10]):  # ìƒìœ„ 10ê°œë§Œ
            try:
                # ì œëª©
                title_elem = article.find('h2', class_='post-block__title')
                title = title_elem.get_text(strip=True) if title_elem else "No Title"
                
                # ë§í¬
                link_elem = article.find('a', class_='post-block__title__link')
                link = link_elem['href'] if link_elem else ""
                
                # ìš”ì•½
                excerpt_elem = article.find('div', class_='post-block__content')
                excerpt = excerpt_elem.get_text(strip=True) if excerpt_elem else ""
                
                # ì‘ì„±ì
                author_elem = article.find('span', class_='river-byline__authors')
                author = author_elem.get_text(strip=True) if author_elem else "Unknown"
                
                # ì‹œê°„
                time_elem = article.find('time')
                publish_time = time_elem.get_text(strip=True) if time_elem else ""
                
                startup_data = {
                    'title': title,
                    'link': link,
                    'excerpt': excerpt,
                    'author': author,
                    'publish_time': publish_time,
                    'source': 'TechCrunch',
                    'crawled_at': datetime.now().isoformat()
                }
                
                startups.append(startup_data)
                print(f"âœ… {title[:50]}...")
                
            except Exception as e:
                print(f"âŒ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                continue
            
            # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
            time.sleep(0.5)
        
        # ë°ì´í„° ì €ì¥
        with open('data/techcrunch_startups.json', 'w', encoding='utf-8') as f:
            json.dump(startups, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ‰ TechCrunch í¬ë¡¤ë§ ì™„ë£Œ: {len(startups)}ê°œ ê¸°ì‚¬")
        return startups
        
    except Exception as e:
        print(f"âŒ TechCrunch í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

def crawl_startup_directories():
    """ìŠ¤íƒ€íŠ¸ì—… ë””ë ‰í† ë¦¬ ì‚¬ì´íŠ¸ì—ì„œ ì •ë³´ í¬ë¡¤ë§"""
    print("ğŸ“ ìŠ¤íƒ€íŠ¸ì—… ë””ë ‰í† ë¦¬ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹œì‘...")
    
    # ê°„ë‹¨í•œ ìŠ¤íƒ€íŠ¸ì—… ì •ë³´ (ì‹¤ì œ í¬ë¡¤ë§ ëŒ€ì‹  ìƒ˜í”Œ ë°ì´í„°)
    sample_startups = [
        {
            "name": "OpenAI",
            "description": "AI ì—°êµ¬ ë° ê°œë°œ ê¸°ì—…",
            "location": "San Francisco, CA",
            "industry": "Artificial Intelligence",
            "founded": "2015",
            "funding": "Series Unknown",
            "source": "Sample Data",
            "crawled_at": datetime.now().isoformat()
        },
        {
            "name": "Stripe",
            "description": "ì˜¨ë¼ì¸ ê²°ì œ ì²˜ë¦¬ í”Œë«í¼",
            "location": "San Francisco, CA",
            "industry": "Fintech",
            "founded": "2010",
            "funding": "Series Unknown",
            "source": "Sample Data",
            "crawled_at": datetime.now().isoformat()
        },
        {
            "name": "Notion",
            "description": "í˜‘ì—… ë° ìƒì‚°ì„± ë„êµ¬",
            "location": "San Francisco, CA",
            "industry": "Productivity",
            "founded": "2013",
            "funding": "Series Unknown",
            "source": "Sample Data",
            "crawled_at": datetime.now().isoformat()
        }
    ]
    
    # ë°ì´í„° ì €ì¥
    with open('data/sample_startups.json', 'w', encoding='utf-8') as f:
        json.dump(sample_startups, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ‰ ìƒ˜í”Œ ìŠ¤íƒ€íŠ¸ì—… ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(sample_startups)}ê°œ")
    return sample_startups

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ•·ï¸ ìŠ¤íƒ€íŠ¸ì—… ì •ë³´ í¬ë¡¤ë§ ì‹œì‘")
    print("=" * 50)
    
    # 1. TechCrunch ë‰´ìŠ¤ í¬ë¡¤ë§
    techcrunch_data = crawl_startup_news()
    
    # 2. ìƒ˜í”Œ ìŠ¤íƒ€íŠ¸ì—… ë°ì´í„° ìƒì„±
    sample_data = crawl_startup_directories()
    
    # 3. ì „ì²´ ë°ì´í„° í†µí•©
    all_data = techcrunch_data + sample_data
    
    # ì „ì²´ ë°ì´í„° ì €ì¥
    with open('data/all_crawled_data.json', 'w', encoding='utf-8') as f:
        json.dump(all_data, f, ensure_ascii=False, indent=2)
    
    print("=" * 50)
    print(f"ğŸ¯ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ: {len(all_data)}ê°œ í•­ëª©")
    print("ğŸ“ ë°ì´í„° ì €ì¥ ìœ„ì¹˜: data-pipeline/data/")

if __name__ == "__main__":
    main()
