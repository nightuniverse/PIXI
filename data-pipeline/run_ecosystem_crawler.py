#!/usr/bin/env python3
"""
ìŠ¤íƒ€íŠ¸ì—… ìƒíƒœê³„ íƒìƒ‰ í†µí•© í¬ë¡¤ëŸ¬ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
ì „ì²´ ìƒíƒœê³„ì™€ í•œêµ­ ìƒíƒœê³„ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

import asyncio
import json
import logging
import os
from datetime import datetime
from typing import Dict, Any

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ecosystem_crawler_run.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

async def run_global_ecosystem_crawler():
    """ì „ì²´ ìƒíƒœê³„ í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
    logger.info("ğŸŒ ì „ì²´ ìƒíƒœê³„ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹œì‘")
    
    try:
        from crawlers.ecosystem_crawler import EcosystemCrawler
        
        async with EcosystemCrawler() as crawler:
            ecosystem_data = await crawler.crawl_all_sources()
            filename = crawler.save_data()
            
            if filename:
                logger.info(f"âœ… ì „ì²´ ìƒíƒœê³„ í¬ë¡¤ë§ ì™„ë£Œ: {filename}")
                return ecosystem_data
            else:
                logger.error("âŒ ì „ì²´ ìƒíƒœê³„ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨")
                return None
                
    except Exception as e:
        logger.error(f"ì „ì²´ ìƒíƒœê³„ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return None

async def run_korean_ecosystem_crawler():
    """í•œêµ­ ìƒíƒœê³„ í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
    logger.info("ğŸ‡°ğŸ‡· í•œêµ­ ìƒíƒœê³„ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹œì‘")
    
    try:
        from crawlers.korean_ecosystem_crawler import KoreanEcosystemCrawler
        
        async with KoreanEcosystemCrawler() as crawler:
            korean_ecosystem_data = await crawler.crawl_all_korean_sources()
            filename = crawler.save_data()
            
            if filename:
                logger.info(f"âœ… í•œêµ­ ìƒíƒœê³„ í¬ë¡¤ë§ ì™„ë£Œ: {filename}")
                return korean_ecosystem_data
            else:
                logger.error("âŒ í•œêµ­ ìƒíƒœê³„ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨")
                return None
                
    except Exception as e:
        logger.error(f"í•œêµ­ ìƒíƒœê³„ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return None

def merge_ecosystem_data(global_data: Dict[str, Any], korean_data: Dict[str, Any]) -> Dict[str, Any]:
    """ì „ì²´ ìƒíƒœê³„ì™€ í•œêµ­ ìƒíƒœê³„ ë°ì´í„° í†µí•©"""
    logger.info("ğŸ”„ ìƒíƒœê³„ ë°ì´í„° í†µí•© ì‹œì‘")
    
    merged_data = {
        'global_ecosystem': global_data,
        'korean_ecosystem': korean_data,
        'merged_at': datetime.now().isoformat()
    }
    
    # í†µí•© í†µê³„ ê³„ì‚°
    if global_data and korean_data:
        global_stats = global_data.get('statistics', {})
        korean_stats = korean_data.get('statistics', {})
        
        merged_stats = {
            'global_total_entities': global_stats.get('total_entities', 0),
            'korean_total_entities': korean_stats.get('total_entities', 0),
            'total_unique_startups': len(set(
                [s['name'].lower() for s in global_data.get('startups', [])] +
                [s['name'].lower() for s in korean_data.get('startups', [])]
            )),
            'total_unique_investors': len(set(
                [i['name'].lower() for i in global_data.get('investors', [])] +
                [i['name'].lower() for i in korean_data.get('investors', [])]
            )),
            'total_unique_accelerators': len(set(
                [a['name'].lower() for a in global_data.get('accelerators', [])] +
                [a['name'].lower() for a in korean_data.get('accelerators', [])]
            )),
            'total_unique_coworking_spaces': len(set(
                [c['name'].lower() for c in global_data.get('coworking_spaces', [])] +
                [c['name'].lower() for c in korean_data.get('coworking_spaces', [])]
            )),
            'total_unique_events': len(set(
                [e['name'].lower() for e in global_data.get('events', [])] +
                [e['name'].lower() for e in korean_data.get('events', [])]
            ))
        }
        
        merged_data['merged_statistics'] = merged_stats
        logger.info(f"í†µí•© í†µê³„ ê³„ì‚° ì™„ë£Œ: ì´ {merged_stats['total_unique_startups']}ê°œ ê³ ìœ  ìŠ¤íƒ€íŠ¸ì—…")
    
    return merged_data

def save_merged_data(merged_data: Dict[str, Any], filename: str = None):
    """í†µí•© ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"merged_ecosystem_data_{timestamp}.json"
    
    filepath = f"data/{filename}"
    
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(merged_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"í†µí•© ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filepath}")
        return filepath
        
    except Exception as e:
        logger.error(f"í†µí•© ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
        return None

def generate_summary_report(merged_data: Dict[str, Any]):
    """í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
    logger.info("ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±")
    
    if not merged_data:
        logger.warning("ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        return
    
    global_data = merged_data.get('global_ecosystem', {})
    korean_data = merged_data.get('korean_ecosystem', {})
    merged_stats = merged_data.get('merged_statistics', {})
    
    print("\n" + "="*80)
    print("ğŸš€ ìŠ¤íƒ€íŠ¸ì—… ìƒíƒœê³„ íƒìƒ‰ í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½")
    print("="*80)
    
    if global_data:
        global_stats = global_data.get('statistics', {})
        print(f"\nğŸŒ ì „ì²´ ìƒíƒœê³„ ë°ì´í„°:")
        print(f"   - ìŠ¤íƒ€íŠ¸ì—…: {global_stats.get('total_startups', 0):,}ê°œ")
        print(f"   - íˆ¬ìì: {global_stats.get('total_investors', 0):,}ê°œ")
        print(f"   - ì•¡ì…€ëŸ¬ë ˆì´í„°: {global_stats.get('total_accelerators', 0):,}ê°œ")
        print(f"   - ì½”ì›Œí‚¹ ìŠ¤í˜ì´ìŠ¤: {global_stats.get('total_coworking_spaces', 0):,}ê°œ")
        print(f"   - ì´ë²¤íŠ¸: {global_stats.get('total_events', 0):,}ê°œ")
        print(f"   - ì´ ì—”í‹°í‹°: {global_stats.get('total_entities', 0):,}ê°œ")
    
    if korean_data:
        korean_stats = korean_data.get('statistics', {})
        print(f"\nğŸ‡°ğŸ‡· í•œêµ­ ìƒíƒœê³„ ë°ì´í„°:")
        print(f"   - ìŠ¤íƒ€íŠ¸ì—…: {korean_stats.get('total_startups', 0):,}ê°œ")
        print(f"   - íˆ¬ìì: {korean_stats.get('total_investors', 0):,}ê°œ")
        print(f"   - ì•¡ì…€ëŸ¬ë ˆì´í„°: {korean_stats.get('total_accelerators', 0):,}ê°œ")
        print(f"   - ì½”ì›Œí‚¹ ìŠ¤í˜ì´ìŠ¤: {korean_stats.get('total_coworking_spaces', 0):,}ê°œ")
        print(f"   - ë‰´ìŠ¤: {korean_stats.get('total_news', 0):,}ê°œ")
        print(f"   - ì´ ì—”í‹°í‹°: {korean_stats.get('total_entities', 0):,}ê°œ")
    
    if merged_stats:
        print(f"\nğŸ”„ í†µí•© ë°ì´í„° ìš”ì•½:")
        print(f"   - ê³ ìœ  ìŠ¤íƒ€íŠ¸ì—…: {merged_stats.get('total_unique_startups', 0):,}ê°œ")
        print(f"   - ê³ ìœ  íˆ¬ìì: {merged_stats.get('total_unique_investors', 0):,}ê°œ")
        print(f"   - ê³ ìœ  ì•¡ì…€ëŸ¬ë ˆì´í„°: {merged_stats.get('total_unique_accelerators', 0):,}ê°œ")
        print(f"   - ê³ ìœ  ì½”ì›Œí‚¹ ìŠ¤í˜ì´ìŠ¤: {merged_stats.get('total_unique_coworking_spaces', 0):,}ê°œ")
        print(f"   - ê³ ìœ  ì´ë²¤íŠ¸: {merged_stats.get('total_unique_events', 0):,}ê°œ")
    
    print(f"\nğŸ“… í¬ë¡¤ë§ ì™„ë£Œ ì‹œê°„: {merged_data.get('merged_at', 'Unknown')}")
    print("="*80)

def run_rss_korean_crawler():
    """RSS ì „ìš© í•œêµ­ í¬ë¡¤ëŸ¬ (Playwright ë¶ˆí•„ìš”, ì•ˆì •ì )"""
    logger.info("ğŸ“¡ RSS ê¸°ë°˜ í•œêµ­ í¬ë¡¤ëŸ¬ ì‹¤í–‰ (ìš°ì„  ì‹œë„)")
    try:
        from crawlers.rss_ecosystem_crawler import run_rss_crawler
        from crawlers.public_data_loader import load_csv_dir, merge_into_ecosystem
        data = run_rss_crawler()
        public_items = load_csv_dir("data", pattern="K_STARTUP")
        if public_items:
            data = merge_into_ecosystem(public_items, data)
        return data
    except Exception as e:
        logger.warning("RSS í¬ë¡¤ëŸ¬ ì‹¤íŒ¨: %s", e)
        return None


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("ğŸš€ ìŠ¤íƒ€íŠ¸ì—… ìƒíƒœê³„ íƒìƒ‰ í†µí•© í¬ë¡¤ëŸ¬ ì‹œì‘")
    
    # ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('data', exist_ok=True)
    os.makedirs('logs', exist_ok=True)
    
    try:
        # 0. RSS ê¸°ë°˜ í•œêµ­ ë°ì´í„° ë¨¼ì € (ì‹¤íŒ¨ ê°€ëŠ¥ì„± ë‚®ìŒ)
        korean_data = run_rss_korean_crawler()
        if korean_data:
            ts = datetime.now().strftime("%Y%m%d_%H%M%S")
            path = f"data/korean_ecosystem_data_{ts}.json"
            with open(path, "w", encoding="utf-8") as f:
                json.dump(korean_data, f, ensure_ascii=False, indent=2)
            with open("data/korean_ecosystem_data_latest.json", "w", encoding="utf-8") as f:
                json.dump(korean_data, f, ensure_ascii=False, indent=2)
            logger.info("RSS í•œêµ­ ë°ì´í„° ì €ì¥: %s", path)
        
        # 1. ì „ì²´ ìƒíƒœê³„ í¬ë¡¤ë§ (ì„ íƒ, ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ)
        global_data = await run_global_ecosystem_crawler()
        
        # 2. í•œêµ­ Playwright í¬ë¡¤ë§ì€ RSS ì‹¤íŒ¨ ì‹œì—ë§Œ
        if not korean_data or (korean_data.get("statistics", {}).get("total_startups", 0) < 5):
            korean_playwright = await run_korean_ecosystem_crawler()
            if korean_playwright:
                korean_data = korean_playwright
        else:
            logger.info("í•œêµ­ ë°ì´í„°ëŠ” RSS ê²°ê³¼ ì‚¬ìš© (Playwright ìƒëµ)")
        
        if not korean_data:
            korean_data = {"startups": [], "accelerators": [], "coworking_spaces": [], "statistics": {}}
        
        # 3. ë°ì´í„° í†µí•©
        if global_data or korean_data:
            merged_data = merge_ecosystem_data(global_data or {}, korean_data or {})
            
            # 4. í†µí•© ë°ì´í„° ì €ì¥
            merged_filename = save_merged_data(merged_data)
            
            if merged_filename:
                logger.info(f"âœ… í†µí•© ë°ì´í„° ì €ì¥ ì™„ë£Œ: {merged_filename}")
                
                # 5. ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
                generate_summary_report(merged_data)
                
                logger.info("ğŸ‰ ëª¨ë“  í¬ë¡¤ë§ ì‘ì—… ì™„ë£Œ!")
            else:
                logger.error("âŒ í†µí•© ë°ì´í„° ì €ì¥ ì‹¤íŒ¨")
        else:
            logger.warning("âš ï¸ í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

if __name__ == "__main__":
    asyncio.run(main())
